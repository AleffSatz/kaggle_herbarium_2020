{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking the code works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['http_proxy'] = \"http://proxy.hcm.fpt.vn:80/\"\n",
    "os.environ['https_proxy'] = \"https://proxy.hcm.fpt.vn:80/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                    # Array, Linear Algebra\n",
    "from torch.utils.data.dataset import random_split     # spliting inTrain Val\n",
    "import pandas as pd                                   # handling CSV\n",
    "import os                                             # For File handling\n",
    "import random                                         # Choosing from images dataset\n",
    "import time                                           # timing Epochs  \n",
    "from tqdm.notebook import tqdm                        # Testing\n",
    "from os.path import join                              # File Handling\n",
    "from torchvision import transforms                    # Data Aug\n",
    "import torch                                          # Framework\n",
    "from PIL import Image                                 # Loading Image\n",
    "from torch.utils.data import Dataset, DataLoader      # Dataset\n",
    "import torch.nn.functional as F                       # Function\n",
    "import json                                           # Loading Metadat\n",
    "from PIL import  ImageOps                             # Data Aug \n",
    "from PIL.Image import open as openIm                  # Image Handling\n",
    "import matplotlib.pyplot  as plt                      # Ploting Image\n",
    "import cv2\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the following line to import the functions from few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/bigdata/user/hieunt124/submodules/few_shot/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reproduce Omniglot results of Snell et al Prototypical networks.\n",
    "\"\"\"\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "\n",
    "from few_shot.datasets import OmniglotDataset, MiniImageNet\n",
    "from few_shot.models import get_few_shot_encoder\n",
    "from few_shot.core import NShotTaskSampler, EvaluateFewShot, prepare_nshot_task\n",
    "from few_shot.proto import proto_net_episode\n",
    "from few_shot.train import fit\n",
    "from few_shot.callbacks import *\n",
    "from few_shot.utils import setup_dirs\n",
    "from config import PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import vision stuff\n",
    "'''\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.set_num_threads(8)\n",
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/bigdata/user/hieunt124/kaggle/herbarium/'\n",
    "train_dir = base_dir + '/nybg2020/train/'\n",
    "test_dir = base_dir + '/nybg2020/test/'\n",
    "metadata_file = 'metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76407</th>\n",
       "      <td>0</td>\n",
       "      <td>626762</td>\n",
       "      <td>images/000/00/626762.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601590</th>\n",
       "      <td>0</td>\n",
       "      <td>72077</td>\n",
       "      <td>images/000/00/72077.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76408</th>\n",
       "      <td>0</td>\n",
       "      <td>818271</td>\n",
       "      <td>images/000/00/818271.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556748</th>\n",
       "      <td>0</td>\n",
       "      <td>495523</td>\n",
       "      <td>images/000/00/495523.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335261</th>\n",
       "      <td>0</td>\n",
       "      <td>437000</td>\n",
       "      <td>images/000/00/437000.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category_id      id                 file_name  height  width\n",
       "76407             0  626762  images/000/00/626762.jpg    1000    681\n",
       "601590            0   72077   images/000/00/72077.jpg    1000    681\n",
       "76408             0  818271  images/000/00/818271.jpg    1000    681\n",
       "556748            0  495523  images/000/00/495523.jpg    1000    681\n",
       "335261            0  437000  images/000/00/437000.jpg    1000    681"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(train_dir + metadata_file, encoding = \"ISO-8859-1\") as json_file:\n",
    "    train_metadata = json.load(json_file)\n",
    "\n",
    "train_img = pd.DataFrame(train_metadata['images'])\n",
    "train_label = pd.DataFrame(train_metadata['annotations'])\n",
    "train_df = (pd.merge(train_label, train_img\n",
    "                    #, left_on='image_id'\n",
    "                    , on='id'\n",
    "                    , how='left')\n",
    "            .drop(['image_id', 'license', 'region_id'], axis=1)\n",
    "            .sort_values(by=['category_id'])\n",
    "           )\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.rename(columns={'category_id': 'class_id'\n",
    "                        , 'file_name': 'filepath'\n",
    "                        }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the training parameters. For now, we can keep them as is, just to check the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_dirs()\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "##############\n",
    "# Parameters #\n",
    "##############\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='miniImageNet')\n",
    "parser.add_argument('--distance', default='l2')\n",
    "parser.add_argument('--n-train', default=5, type=int)\n",
    "parser.add_argument('--n-test', default=5, type=int)\n",
    "parser.add_argument('--k-train', default=20, type=int)\n",
    "parser.add_argument('--k-test', default=5, type=int)\n",
    "parser.add_argument('--q-train', default=15, type=int)\n",
    "parser.add_argument('--q-test', default=1, type=int)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset = 'Herbarium'\n",
    "args.q_train = 1\n",
    "args.q_test = 1\n",
    "args.n_train = 1\n",
    "args.n_test = 1\n",
    "args.k_train = 40\n",
    "args.k_test = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we are to make the most changes, as we'll feed a new Herbarium dataset into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we'll omit the classes with only 1 sample, which from value_counts include 3 classes. The remaining classes contain at least 2 samples so we can still perform multi-way, 1-shot 1-query training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "def load_rgb_image(image_file):\n",
    "    '''\n",
    "    load image file in RGB format\n",
    "    '''\n",
    "    img = cv2.imread(str(image_file))\n",
    "    try:\n",
    "        #img = img.astype('uint8')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        print(image_file)\n",
    "        print(img.shape)\n",
    "    return img\n",
    "\n",
    "def get_augmentations(re_size=300#224\n",
    "                      , crop_size=300#224\n",
    "                      , train=True\n",
    "                     ):\n",
    "    '''\n",
    "    get image augmentations from albumentations\n",
    "    '''\n",
    "    augs = [A.Resize(height=re_size, width=re_size)]\n",
    "    if train:\n",
    "        augs.extend([A.RandomCrop(height=crop_size, width=crop_size)\n",
    "                     , A.ShiftScaleRotate(shift_limit=.1, scale_limit=.3, rotate_limit=30, p=.75)\n",
    "                     , A.RandomBrightnessContrast(brightness_limit=.5, contrast_limit=.5, p=.75)\n",
    "                     #, A.Blur(.5)\n",
    "                     , A.Cutout(max_h_size=crop_size//12, max_w_size=crop_size//12, p=.5)\n",
    "                    ])\n",
    "    else:\n",
    "        augs.extend([A.CenterCrop(height=crop_size, width=crop_size)])\n",
    "    \n",
    "    # A.Normalize uses Imagenet stats by default\n",
    "    return A.Compose(augs + [A.Normalize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myHerbariumDataset(Dataset\n",
    "                        ):\n",
    "    def __init__(self, df_image\n",
    "                         , train=True\n",
    "                         , aug=True\n",
    "                         , base_folder='/bigdata/user/hieunt124/kaggle/herbarium/'):\n",
    "        if (train):\n",
    "            df_image.index = df_image['id']\n",
    "\n",
    "        self.df = df_image ## dataframe of all image annotations\n",
    "        self.datasetid_to_filepath = df_image.to_dict()['filepath']  ## get file path from image id\n",
    "        self.datasetid_to_class_id = df_image.to_dict()['class_id']  ## get class id from image id\n",
    "\n",
    "        self.classes = self.df['class_id'].unique() ## list of labels\n",
    "        self.base_folder = base_folder\n",
    "        self.loader = lambda x: load_rgb_image(base_folder + x)  ## loader function for the image\n",
    "        self.transform = get_augmentations(train=aug) ## transform the image\n",
    "        self.to_tensor = transforms.ToTensor()  ## transform image to Torch tensor\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        '''\n",
    "        input item id, output the image and its label\n",
    "        '''\n",
    "        \n",
    "        image = self.loader(self.datasetid_to_filepath[item])\n",
    "        image = self.transform(image=image)['image']\n",
    "        image = self.to_tensor(image)\n",
    "        label = self.datasetid_to_class_id[item]\n",
    "        \n",
    "        return image, label\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        returns size of dataset\n",
    "        '''\n",
    "        return len(self.df)\n",
    "    \n",
    "    def num_classes(self):\n",
    "        '''\n",
    "        returns number of classes\n",
    "        '''\n",
    "        return len(self.classes)\n",
    "    \n",
    "    def classes_value_counts(self):\n",
    "        '''\n",
    "        return number of samples per class\n",
    "        '''\n",
    "        return self.df.Label.value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll extract the classes that only have 1 sample here, they'll be excluded from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23142, 24021, 22922])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = pd.DataFrame(train_df.class_id.value_counts().reset_index()\n",
    "                           )\n",
    "label_counts.columns = ['class_id','count_samples']\n",
    "single_categories = label_counts[label_counts['count_samples'] < 2].class_id.values\n",
    "single_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the Dataset class, we can then define the corresponding data loaders. The number of episodes is taken to be roughly the number of total classes divided by number of classes sampled for each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12000it [00:00, 135477.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_episodes = 1000\n",
    "episodes_per_epoch = 800\n",
    "\n",
    "if args.dataset == 'omniglot':\n",
    "    n_epochs = 40\n",
    "    dataset_class = OmniglotDataset\n",
    "    num_input_channels = 1\n",
    "    drop_lr_every = 20\n",
    "elif args.dataset == 'miniImageNet':\n",
    "    n_epochs = 35 #120\n",
    "    dataset_class = MiniImageNet\n",
    "    num_input_channels = 3\n",
    "    drop_lr_every = 40\n",
    "else:\n",
    "    n_epochs = 24 #120\n",
    "    #dataset_class = MiniImageNet\n",
    "    num_input_channels = 3\n",
    "    drop_lr_every = 40\n",
    "    #raise(ValueError, 'Unsupported dataset')\n",
    "\n",
    "param_str = f'{args.dataset}_nt={args.n_train}_kt={args.k_train}_qt={args.q_train}_' \\\n",
    "            f'nv={args.n_test}_kv={args.k_test}_qv={args.q_test}'\n",
    "\n",
    "print(param_str)\n",
    "\n",
    "###################\n",
    "# Create datasets #\n",
    "###################\n",
    "#background = dataset_class('background')\n",
    "background = myHerbariumDataset(train_df[-train_df.class_id.isin(single_categories)]\n",
    "                                , train=True\n",
    "                                , base_folder=train_dir \n",
    "                               )\n",
    "background_taskloader = DataLoader(\n",
    "    background,\n",
    "    batch_sampler=NShotTaskSampler(background, episodes_per_epoch, args.n_train, args.k_train, args.q_train),\n",
    "    num_workers=4\n",
    ")\n",
    "evaluation = MiniImageNet('evaluation')\n",
    "evaluation_taskloader = DataLoader(\n",
    "    evaluation,\n",
    "    batch_sampler=NShotTaskSampler(evaluation, episodes_per_epoch, args.n_test, args.k_test, args.q_test),\n",
    "    num_workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we use the model architecture predefined as in few_shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrainedmodels import se_resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.layers import AdaptiveConcatPool2d, Flatten\n",
    "\n",
    "def ResnetProtoTypeNet():\n",
    "    \n",
    "    def my_head(input_size, hidden_units, output_size):\n",
    "        return nn.Sequential(AdaptiveConcatPool2d()\n",
    "                                        , Flatten()\n",
    "                                        , nn.BatchNorm1d(num_features=2 * input_size)\n",
    "                                        , nn.Dropout(p=.25)\n",
    "                                        , nn.Linear(in_features=2 * input_size, out_features=hidden_units, bias=True)\n",
    "                                        , nn.ReLU(inplace=True)\n",
    "                                        , nn.BatchNorm1d(num_features=hidden_units)\n",
    "                                        , nn.Dropout(p=.5)\n",
    "                                        , nn.Linear(in_features=hidden_units, out_features=output_size, bias=True)\n",
    "                                        \n",
    "                                       )\n",
    "\n",
    "    arch = se_resnet101(pretrained=None)\n",
    "    arch = list(arch.children())\n",
    "    arch.pop(-1)\n",
    "    arch.pop(-1)\n",
    "    temp_arch = nn.Sequential(nn.Sequential(*arch))\n",
    "    temp_children = list(temp_arch.children())\n",
    "    temp_children.append(my_head(2048, 512, 200))\n",
    "    model = nn.Sequential(*temp_children)\n",
    "    \n",
    "    model_dir = '/bigdata/user/hieunt124/kaggle/herbarium/nybg2020/train/models/'\n",
    "    model_file = 'herbarium-seresnet101-weights.pth'\n",
    "    weights = torch.load(model_dir + model_file)\n",
    "\n",
    "    model.load_state_dict(weights['state_dict'])\n",
    "\n",
    "    temp_head = list(model.children())[-1]\n",
    "    # temp_head = nn.Sequential(*list(temp_head.children())[:-2])\n",
    "    temp_head = nn.Sequential(*list(temp_head.children())[:2])\n",
    "    temp_arch = nn.Sequential(nn.Sequential(*list(model.children())[:-1]))\n",
    "    model = nn.Sequential(temp_arch, temp_head)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arch = ResnetPrototypeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (3): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (3): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (4): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (5): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (7): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (8): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (9): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (10): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (11): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (12): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (13): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (14): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (15): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (16): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (17): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (18): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (19): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (20): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (21): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (22): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): AdaptiveConcatPool2d(\n",
       "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "    )\n",
       "    (1): Flatten()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#########\n",
    "# Model #\n",
    "#########\n",
    "#model = get_few_shot_encoder(num_input_channels)\n",
    "#model.to(device, dtype=torch.double)\n",
    "model = ResnetProtoTypeNet()\n",
    "model.to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ADAM optimizer and Negative log-likelihood loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Prototypical network on Herbarium...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############\n",
    "# Training #\n",
    "############\n",
    "print(f'Training Prototypical network on {args.dataset}...')\n",
    "optimiser = Adam(model.parameters(), lr=1e-3)\n",
    "#loss_fn = torch.nn.NLLLoss().cuda()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    # Drop lr every 2000 episodes\n",
    "    if epoch % drop_lr_every == 0:\n",
    "        return lr / 2\n",
    "    else:\n",
    "        return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EvaluateFewShot(\n",
    "        eval_fn=proto_net_episode,\n",
    "        num_tasks=evaluation_episodes,\n",
    "        n_shot=args.n_test,\n",
    "        k_way=args.k_test,\n",
    "        q_queries=args.q_test,\n",
    "        taskloader=evaluation_taskloader,\n",
    "        prepare_batch=prepare_nshot_task(args.n_test, args.k_test, args.q_test),\n",
    "        distance=args.distance\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=PATH + f'/models/proto_nets/{param_str}.pth',\n",
    "        monitor=f'val_{args.n_test}-shot_{args.k_test}-way_acc'\n",
    "    ),\n",
    "    LearningRateScheduler(schedule=lr_schedule),\n",
    "    CSVLogger(PATH + f'/logs/proto_nets/{param_str}.csv'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fit(\n",
    "    model,\n",
    "    optimiser,\n",
    "    loss_fn,\n",
    "    epochs=n_epochs,\n",
    "    dataloader=background_taskloader,\n",
    "    prepare_batch=prepare_nshot_task(args.n_train, args.k_train, args.q_train),\n",
    "    callbacks=callbacks,\n",
    "    metrics=['categorical_accuracy'],\n",
    "    fit_function=proto_net_episode,\n",
    "    fit_function_kwargs={'n_shot': args.n_train, 'k_way': args.k_train, 'q_queries': args.q_train, 'train': True,\n",
    "                         'distance': args.distance},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained the model, let's try to test the model on the test set. For this, we'll create dataloaders for both train and test sets. We use the train dataloader to compute the prototypes for each class. We then determine the class of samples in the test set by taking the class with the minimal distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miniImageNet_nt=5_kt=20_qt=15_nv=5_kv=5_qv=1.pth',\n",
       " 'miniImageNet_nt=1_kt=60_qt=5_nv=1_kv=5_qv=1.pth',\n",
       " 'miniImageNet_nt=1_kt=50_qt=1_nv=1_kv=10_qv=1.pth',\n",
       " 'Herbarium_nt=1_kt=30_qt=1_nv=1_kv=10_qv=1_seresnet101.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1_resnet.pth',\n",
       " 'Herbarium_nt=1_kt=60_qt=1_nv=1_kv=10_qv=1_resnet.pth',\n",
       " 'Herbarium_nt=1_kt=60_qt=1_nv=1_kv=10_qv=1_resnet_epoch30_cat790.pth',\n",
       " 'Herbarium_nt=1_kt=50_qt=1_nv=1_kv=10_qv=1.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1_epoch=40.pth',\n",
       " 'Herbarium_nt=1_kt=45_qt=1_nv=1_kv=10_qv=1_resnet101_mixup.pth',\n",
       " 'miniImageNet_nt=5_kt=60_qt=5_nv=5_kv=5_qv=5.pth',\n",
       " 'miniImageNet_nt=5_kt=60_qt=5_nv=1_kv=5_qv=1.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1_resnet_epoch20_cat725.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1.pth',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/bigdata/user/hieunt124/submodules/few_shot/models/proto_nets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '/bigdata/user/hieunt124/submodules/few_shot/models/proto_nets/Herbarium_nt=1_kt=30_qt=1_nv=1_kv=10_qv=1_seresnet101.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-create the dataset object, this time with all the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to jack up batchsize a bit if the GPU can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support_df = train_df.reset_index(drop=True)\n",
    "support_df = train_df[train_df.class_id.isin(np.arange(1000))].reset_index(drop=True)\n",
    "support_dataset = myHerbariumDataset(support_df\n",
    "                                     , train=False\n",
    "                                     , aug=False\n",
    "                                     , base_folder=train_dir)\n",
    "support_loader = DataLoader(support_dataset, batch_size=64\n",
    "                            , shuffle=False\n",
    "                            , pin_memory=True\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = support_df.sample(4096).copy().reset_index(drop=True)\n",
    "test_dataset = myHerbariumDataset(test_df\n",
    "                                  , train=False\n",
    "                                  , aug=False\n",
    "                                  , base_folder=train_dir\n",
    "                                 )\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False\n",
    "                         , num_workers=0\n",
    "                         , pin_memory=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class PrototypeSupportLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we loop through the dataloader, we save the embeddings and corresponding labels. The dataset is large so it's no quite viable to load all of the embeddings into a matrix, I'm thinking of how we can update the class prototype on the fly instead. So at any point, hopefully, we'll just need: 32k * 12k + 64 * 12k numbers, as opposed to 1m * 12k which is exhausting. We'll also need to keep track of how many samples have been used so far for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better yet, we can pre-sort the support dataset by class then write a custom dataloader which draws batch size according to how many samples there are for each class. This way, we won't have to compute loops within loops and can compute prototypes class by class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we'll define a Prototype class which will take as input the pretrained model, the annotations dataframe and the (custom) dataloader based on which it computes the class prototypes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating, this Prototype object takes as input the evaluation dataloader, then computes the embeddings for each sample in this val/test set. To save memory, for each sample, we'll save only the top-1 or top-5 least distances and the respective predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there's gonna be a shit ton of distances to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder if there's any merit in creating a ClassLoader and loop through that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prototypes():\n",
    "    def __init__(self, model, df, support_loader, device=torch.device('cuda')):\n",
    "        self.df = df\n",
    "        self.n_classes = self.df['class_id'].nunique()\n",
    "        self.support_loader = support_loader\n",
    "        self.model = model\n",
    "        self.classes = self.df['class_id'].unique().tolist()\n",
    "        self.device=device\n",
    "        self.model.to(device)\n",
    "        self._get_prototypes()\n",
    "        \n",
    "    \n",
    "    def _get_prototypes(self):\n",
    "        '''\n",
    "        compute class prototypes and store in self.class_prototypes \n",
    "        corresponding to class index\n",
    "        '''\n",
    "        #print('Computing prototypes...')\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (X, y) in (enumerate(self.support_loader)):\n",
    "                X, y = X.to(device, non_blocking=True), list(y)\n",
    "                X_embeddings = self.model.float()(X)#.cpu().detach().numpy()\n",
    "                #print(X_embeddings.shape[-1])\n",
    "                if batch_index == 0:\n",
    "                    \n",
    "                    # this matrix will hold the prototypes\n",
    "                    \n",
    "                    #self.class_prototypes = np.zeros((self.n_classes,X_embeddings.shape[-1]))\n",
    "                    self.class_prototypes = torch.zeros((self.n_classes,X_embeddings.shape[-1])).to(self.device)\n",
    "                                                 \n",
    "                    # this array will hold the item tally for each class, \n",
    "                    # this will also be updated on the fly\n",
    "                    #class_items_count = np.zeros(self.n_classes)\n",
    "                    class_items_count = torch.zeros(self.n_classes).to(self.device)\n",
    "                \n",
    "                for i, label in enumerate(y):\n",
    "                    \n",
    "                    label_index = self.classes.index(label)\n",
    "                    temp_item_count = class_items_count[label_index]\n",
    "                    #print(type(self.class_prototypes), type(label_index), type(X_embeddings), type(temp_item_count))\n",
    "                    self.class_prototypes[label_index] = (self.class_prototypes[label_index] * temp_item_count \n",
    "                                                    + X_embeddings[i]) / (temp_item_count+1)\n",
    "                    class_items_count[label_index]+=1\n",
    "                \n",
    "    def _get_embeddings(self, images):\n",
    "        '''\n",
    "        compute embeddings from input images\n",
    "        '''\n",
    "        #X_embeddings = self.model(images.float()).cpu().detach().numpy()\n",
    "        X_embeddings = self.model(images.float())\n",
    "        return X_embeddings\n",
    "    \n",
    "    def _get_predictions(self, embeddings\n",
    "                         , softmax=True\n",
    "                         , normalized_softmax=True\n",
    "                        ):\n",
    "        '''\n",
    "        compute pairwise distances from samples to each of the classes \n",
    "        and retain minimum distances to determine predicted classes\n",
    "        \n",
    "        '''\n",
    "        #preds = np.zeros((embeddings.shape[0], self.n_classes))\n",
    "        #for i, embedding in enumerate(embeddings):\n",
    "        \n",
    "        #for i, prototype in (enumerate(self.class_prototypes)):\n",
    "            #preds[:,i] = np.linalg.norm((embeddings - prototype), axis=1)\n",
    "             #preds[:,i] =                            \n",
    "        \n",
    "        preds = pairwise_euclidean_distance(embeddings, self.class_prototypes)\n",
    "        '''\n",
    "        if softmax:\n",
    "            if normalized_softmax:\n",
    "                preds /= np.max([1.0, np.abs(preds.mean())])\n",
    "            preds = np_softmax(preds)\n",
    "        '''\n",
    "        return preds\n",
    "    def predict(self, test_loader\n",
    "                , proba=True\n",
    "                , **kwargs):\n",
    "        '''\n",
    "        predict \n",
    "        '''\n",
    "        preds_list = []\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_index, (X_test, dummy_target) in tqdm(enumerate(test_loader)):\n",
    "                #time_elapsed = []\n",
    "                #time_elapsed.append(datetime.now())\n",
    "                X_test = X_test.to(device, non_blocking=True)\n",
    "                #time_elapsed.append(datetime.now())\n",
    "                X_embeddings = self._get_embeddings(X_test)\n",
    "                #time_elapsed.append(datetime.now())\n",
    "                temp_preds = self._get_predictions(X_embeddings)\n",
    "                #time_elapsed.append(datetime.now())\n",
    "                preds_list.append(temp_preds)\n",
    "                #time_elapsed.append(datetime.now())\n",
    "                #print(time_elapsed)\n",
    "        if (proba):\n",
    "            #return torch.cat(preds_list)\n",
    "            return np.concatenate(preds_list)\n",
    "        else:\n",
    "            \n",
    "            return ([self.classes[x] for x in torch.argmin(torch.cat(preds_list), axis=1)]\n",
    "                   , np.min(torch.cat(preds_list).cpu().detach().numpy(), axis=1)\n",
    "                   )\n",
    "            '''\n",
    "            return ([self.classes[x] for x in np.argmin(np.concatenate(preds_list), axis=1)]\n",
    "                    #np.argmin(np.concatenate(preds_list), axis=1)\n",
    "                    , np.min(np.concatenate(preds_list), axis=1)\n",
    "                   )\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_euclidean_distance(x, y):\n",
    "    '''\n",
    "    compute pairwise euclidean distance with torch tensors\n",
    "    '''\n",
    "    if not type(x) is torch.Tensor: x = torch.Tensor(x)\n",
    "    if not type(y) is torch.Tensor: y = torch.Tensor(y)\n",
    "        \n",
    "    n_x = x.shape[0]\n",
    "    n_y = y.shape[0]\n",
    "    distances = (\n",
    "                    x.cuda().unsqueeze(1).expand(n_x, n_y, -1) -\n",
    "                    y.cuda().unsqueeze(0).expand(n_x, n_y, -1)\n",
    "            ).pow(2).sum(dim=2)\n",
    "    return distances#.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 5s, sys: 2min 25s, total: 38min 30s\n",
      "Wall time: 10min 36s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%time herbarium_prototypes = Prototypes(model, support_df, support_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.75 GiB (GPU 3; 15.90 GiB total capacity; 11.35 GiB already allocated; 2.68 GiB free; 12.30 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ab15937c6074>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_loader, proba, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m#time_elapsed.append(datetime.now())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mX_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;31m#time_elapsed.append(datetime.now())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mtemp_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ab15937c6074>\u001b[0m in \u001b[0;36m_get_embeddings\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     49\u001b[0m         '''\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#X_embeddings = self.model(images.float()).cpu().detach().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mX_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.75 GiB (GPU 3; 15.90 GiB total capacity; 11.35 GiB already allocated; 2.68 GiB free; 12.30 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "%time predictions = herbarium_prototypes.predict(test_loader, proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hieunt124-fastai/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6844055445805832"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(test_df['class_id'], predictions[0]\n",
    "               , average='macro'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(herbarium_prototypes.class_prototypes[:,0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suppose we can relieve some of memory load by computing prototypes on batches of classes at a time. For one batch of classes, we get a minimal distance and its corresponding class. By the end, we get say 50 candidates of (min distance, class) and just take the minimal of those. We won't be able to get top-5 accuracy this way but I'm struggling to find ways to lighten the memory load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_dataset.num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:10:40,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [35:05,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [35:06,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:41,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:53,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:56,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:46,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:34,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:36,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:30,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:18,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:34,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:22,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:23,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:26,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:37,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:36,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [35:18,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:12,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [34:11,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:01:09,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [2:11:15,  7.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:39:09,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:33:43,  5.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:42:53,  5.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:27:52,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:19:02,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:06:16,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:06:03,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:28:43,  4.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:29:46,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [1:24:12,  4.67s/it]\n"
     ]
    }
   ],
   "source": [
    "n_class_batch = 32\n",
    "np_distance = np.zeros((test_df.shape[0], n_class_batch))\n",
    "np_class = np.zeros((test_df.shape[0], n_class_batch))\n",
    "batch_index = 0\n",
    "for class_batch in (np.array_split(np.arange(train_df.class_id.nunique()), n_class_batch)):\n",
    "    print(batch_index)\n",
    "    temp_support_df = train_df[train_df.class_id.isin(class_batch)].reset_index(drop=True)\n",
    "    support_dataset = myHerbariumDataset(temp_support_df\n",
    "                                         , train=False, aug=False\n",
    "                                         , base_folder=train_dir)\n",
    "    support_loader = DataLoader(support_dataset, batch_size=32\n",
    "                                , shuffle=False\n",
    "                                , num_workers=0\n",
    "                               )\n",
    "    herbarium_prototypes = Prototypes(model, temp_support_df, support_loader)\n",
    "    temp_class, temp_distance = herbarium_prototypes.predict(test_loader=test_loader, proba=False)\n",
    "    np_class[:, batch_index] = temp_class\n",
    "    np_distance[:, batch_index] = temp_distance\n",
    "    batch_index+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh_class = np_class[:5,:5]\n",
    "meh_dist = np_distance[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh_argmin = np.argmin(meh_dist, axis=1)\n",
    "meh_argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[int(meh_class[i, meh_argmin[i]]) for i in range(meh_class.shape[0] )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_argmin = np.argmin(np_distance, axis=1)\n",
    "predicted_labels = [int(np_class[i, class_argmin[i]]) for i in range(np_class.shape[0] )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7292, 1446, 30649, 7331, 8539]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Predicted'] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138292, 8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filepath</th>\n",
       "      <th>height</th>\n",
       "      <th>id</th>\n",
       "      <th>license</th>\n",
       "      <th>width</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60405</td>\n",
       "      <td>images/000/0.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131083</td>\n",
       "      <td>images/000/1.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>673</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122596</td>\n",
       "      <td>images/000/2.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82493</td>\n",
       "      <td>images/000/3.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92157</td>\n",
       "      <td>images/000/4.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index          filepath  height  id  license  width  class_id\n",
       "0   60405  images/000/0.jpg    1000   0        1    667         0\n",
       "1  131083  images/000/1.jpg    1000   1        1    673         0\n",
       "2  122596  images/000/2.jpg    1000   2        1    674         0\n",
       "3   82493  images/000/3.jpg    1000   3        1    667         0\n",
       "4   92157  images/000/4.jpg    1000   4        1    676         0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_dir + metadata_file, encoding = \"ISO-8859-1\") as json_file:\n",
    "    test_metadata = json.load(json_file)\n",
    "\n",
    "test_df = pd.DataFrame(test_metadata['images'])\n",
    "test_df['class_id'] = 0\n",
    "test_df.rename(columns={'file_name': 'filepath'}, inplace=True)\n",
    "test_df.sort_values('id', inplace=True)\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138292, 7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_df = train_df.reset_index(drop=True)\n",
    "support_dataset = myHerbariumDataset(support_df\n",
    "                                     , train=False\n",
    "                                     , base_folder=train_dir)\n",
    "support_loader = DataLoader(support_dataset, batch_size=32\n",
    "                            , shuffle=False\n",
    "                            , num_workers=2\n",
    "                           )\n",
    "test_dataset = myHerbariumDataset(test_df, train=False\n",
    "                                  , aug=False\n",
    "                                  , base_folder=test_dir\n",
    "                                  \n",
    "                                 )\n",
    "test_loader = DataLoader(test_dataset, batch_size=128\n",
    "                         , shuffle=False\n",
    "                         , num_workers=0\n",
    "                         , pin_memory=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "herbarium_prototypes = Prototypes(model, support_df, support_loader)\n",
    "test_df['Predicted'] = herbarium_prototypes.predict(test_loader, proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_copy = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Predicted'] = predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_df[['id','Predicted']]\n",
    " .rename(columns={'id':'Id'})\n",
    " .to_csv(base_dir + 'submission_seresnet_epoch50.csv', index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle competitions submit -c herbarium-2020-fgvc7 -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh_preds = np.zeros((4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh_preds[:,0] = np.linalg.norm(meh, axis=1)\n",
    "meh_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(meh, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (X, y) in tqdm(enumerate(support_loader)):\n",
    "        X, y = X.to(device), list(y)\n",
    "        X_embeddings = model.float()(X).cpu().detach().numpy()\n",
    "        \n",
    "        if batch_index == 0:\n",
    "            # this matrix will hold the prototypes\n",
    "            class_prototypes = np.zeros((train_df.class_id.nunique()\n",
    "                                         ,X_embeddings.shape[-1]))\n",
    "            # this array will hold the item tally for each class, \n",
    "            # this will also be updated on the fly\n",
    "            class_items_count = np.zeros(train_df.class_id.nunique())\n",
    "        for i, label in enumerate(y):\n",
    "            temp_item_count = class_items_count[label]\n",
    "            class_prototypes[label] = (class_prototypes[label] * temp_item_count  + X_embeddings[i]) / (temp_item_count+1)\n",
    "            class_items_count[label]+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "#model.to('cpu')\n",
    "with torch.no_grad():\n",
    "    X_embeddings = model.float()(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embeddings.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably better to store a list of embeddings with corresponding list of classes, rather than storing a gigantic matrix of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh = np.random.randn(len(temp_loader.dataset), X_embeddings.shape[-1])\n",
    "meh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally produce a sample test prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138292"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stuff in tqdm((background_taskloader)):\n",
    "    image, label = stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh = background.df.to_dict()['filepath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[-train_df.class_id.isin(single_categories)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh[meh.id == 383760]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background.datasetid_to_filepath[383760]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background.loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.class_id == 6143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh = load_rgb_image(train_dir + train_df[train_df.id == 383760].filepath.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(meh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
