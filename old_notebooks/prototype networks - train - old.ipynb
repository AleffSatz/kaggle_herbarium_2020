{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we train the prototypical network for  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "os.environ['OMP_NUM_THREADS'] = '6'\n",
    "os.environ['MKL_NUM_THREADS'] = '6'\n",
    "#os.environ['http_proxy'] = \"http://proxy.hcm.fpt.vn:80/\"\n",
    "#os.environ['https_proxy'] = \"https://proxy.hcm.fpt.vn:80/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                    # Array, Linear Algebra\n",
    "from torch.utils.data.dataset import random_split     # spliting inTrain Val\n",
    "import pandas as pd                                   # handling CSV\n",
    "import os                                             # For File handling\n",
    "import random                                         # Choosing from images dataset\n",
    "import time                                           # timing Epochs  \n",
    "from tqdm.notebook import tqdm                        # Testing\n",
    "from os.path import join                              # File Handling\n",
    "from torchvision import transforms                    # Data Aug\n",
    "import torch                                          # Framework\n",
    "from PIL import Image                                 # Loading Image\n",
    "from torch.utils.data import Dataset, DataLoader      # Dataset\n",
    "import torch.nn.functional as F                       # Function\n",
    "import json                                           # Loading Metadat\n",
    "from PIL import  ImageOps                             # Data Aug \n",
    "from PIL.Image import open as openIm                  # Image Handling\n",
    "import matplotlib.pyplot  as plt                      # Ploting Image\n",
    "import cv2\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the following line to import the functions from few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./few_shot/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reproduce Omniglot results of Snell et al Prototypical networks.\n",
    "\"\"\"\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "\n",
    "from few_shot.datasets import OmniglotDataset, MiniImageNet\n",
    "from few_shot.models import get_few_shot_encoder\n",
    "from few_shot.core import NShotTaskSampler, EvaluateFewShot, prepare_nshot_task\n",
    "from few_shot.proto import proto_net_episode\n",
    "from few_shot.train import fit\n",
    "from few_shot.callbacks import *\n",
    "from few_shot.utils import setup_dirs\n",
    "from config import PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import vision stuff\n",
    "'''\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_dir = '/bigdata/user/hieunt124/kaggle/herbarium/'\n",
    "base_dir = 'D:/Data/Kaggle_HerbariumChallenge2020'\n",
    "train_dir = base_dir + '/nybg2020/train/'\n",
    "test_dir = base_dir + '/nybg2020/test/'\n",
    "metadata_file = 'metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76407</th>\n",
       "      <td>0</td>\n",
       "      <td>626762</td>\n",
       "      <td>images/000/00/626762.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601590</th>\n",
       "      <td>0</td>\n",
       "      <td>72077</td>\n",
       "      <td>images/000/00/72077.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76408</th>\n",
       "      <td>0</td>\n",
       "      <td>818271</td>\n",
       "      <td>images/000/00/818271.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556748</th>\n",
       "      <td>0</td>\n",
       "      <td>495523</td>\n",
       "      <td>images/000/00/495523.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335261</th>\n",
       "      <td>0</td>\n",
       "      <td>437000</td>\n",
       "      <td>images/000/00/437000.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category_id      id                 file_name  height  width\n",
       "76407             0  626762  images/000/00/626762.jpg    1000    681\n",
       "601590            0   72077   images/000/00/72077.jpg    1000    681\n",
       "76408             0  818271  images/000/00/818271.jpg    1000    681\n",
       "556748            0  495523  images/000/00/495523.jpg    1000    681\n",
       "335261            0  437000  images/000/00/437000.jpg    1000    681"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(train_dir + metadata_file, encoding = \"ISO-8859-1\") as json_file:\n",
    "    train_metadata = json.load(json_file)\n",
    "\n",
    "train_img = pd.DataFrame(train_metadata['images'])\n",
    "train_label = pd.DataFrame(train_metadata['annotations'])\n",
    "train_df = (pd.merge(train_label, train_img\n",
    "                    #, left_on='image_id'\n",
    "                    , on='id'\n",
    "                    , how='left')\n",
    "            .drop(['image_id', 'license', 'region_id'], axis=1)\n",
    "            .sort_values(by=['category_id'])\n",
    "           )\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.rename(columns={'category_id': 'class_id'\n",
    "                        , 'file_name': 'filepath'\n",
    "                        }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the training parameters. For now, we can keep them as is, just to check the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_dirs()\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "##############\n",
    "# Parameters #\n",
    "##############\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='miniImageNet')\n",
    "parser.add_argument('--distance', default='l2')\n",
    "parser.add_argument('--n-train', default=5, type=int)\n",
    "parser.add_argument('--n-test', default=5, type=int)\n",
    "parser.add_argument('--k-train', default=20, type=int)\n",
    "parser.add_argument('--k-test', default=5, type=int)\n",
    "parser.add_argument('--q-train', default=15, type=int)\n",
    "parser.add_argument('--q-test', default=1, type=int)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset = 'Herbarium'\n",
    "args.q_train = 1\n",
    "args.q_test = 1\n",
    "args.n_train = 1\n",
    "args.n_test = 1\n",
    "args.k_train = 30\n",
    "args.k_test = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we are to make the most changes, as we'll feed a new Herbarium dataset into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we'll omit the classes with only 1 sample, which from value_counts include 3 classes. The remaining classes contain at least 2 samples so we can still perform multi-way, 1-shot 1-query training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "def load_rgb_image(image_file):\n",
    "    '''\n",
    "    load image file in RGB format\n",
    "    '''\n",
    "    img = cv2.imread(str(image_file))\n",
    "    try:\n",
    "        #img = img.astype('uint8')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        print(image_file)\n",
    "        print(img.shape)\n",
    "    return img\n",
    "\n",
    "def get_augmentations(re_size=300#224\n",
    "                      , crop_size=300#224\n",
    "                      , train=True\n",
    "                     ):\n",
    "    '''\n",
    "    get image augmentations from albumentations\n",
    "    '''\n",
    "    augs = [A.Resize(height=re_size, width=re_size)]\n",
    "    if train:\n",
    "        augs.extend([A.RandomCrop(height=crop_size, width=crop_size)\n",
    "                     , A.ShiftScaleRotate(shift_limit=.1, scale_limit=.3, rotate_limit=30, p=.75)\n",
    "                     #, A.RandomBrightnessContrast(brightness_limit=.5, contrast_limit=.5, p=.5)\n",
    "                     , A.RandomBrightnessContrast(brightness_limit=.3, contrast_limit=.3, p=.15)\n",
    "                     #, A.Blur(.5)\n",
    "                     , A.Cutout(max_h_size=crop_size//12, max_w_size=crop_size//12, p=.75)\n",
    "                    ])\n",
    "    else:\n",
    "        augs.extend([A.CenterCrop(height=crop_size, width=crop_size)])\n",
    "    \n",
    "    # A.Normalize uses Imagenet stats by default\n",
    "    return A.Compose(augs + [A.Normalize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myHerbariumDataset(Dataset\n",
    "                        ):\n",
    "    def __init__(self, df_image\n",
    "                         , train=True\n",
    "                         , base_folder='/bigdata/user/hieunt124/kaggle/herbarium/'):\n",
    "        if (train):\n",
    "            df_image.index = df_image['id']\n",
    "\n",
    "        self.df = df_image ## dataframe of all image annotations\n",
    "        self.datasetid_to_filepath = df_image.to_dict()['filepath']  ## get file path from image id\n",
    "        self.datasetid_to_class_id = df_image.to_dict()['class_id']  ## get class id from image id\n",
    "\n",
    "        self.classes = self.df['class_id'].unique() ## list of labels\n",
    "        self.base_folder = base_folder\n",
    "        self.loader = lambda x: load_rgb_image(base_folder + x)  ## loader function for the image\n",
    "        self.transform = get_augmentations(train=train) ## transform the image\n",
    "        self.to_tensor = transforms.ToTensor()  ## transform image to Torch tensor\n",
    "        \n",
    "        self.fastai_transforms = get_transforms(flip_vert=True)\n",
    "        self.fastai_loader = lambda x: get_fastai_img(base_folder + x, self.fastai_transforms)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        '''\n",
    "        input item id, output the image and its label\n",
    "        '''\n",
    "        \n",
    "        #image = self.loader(self.datasetid_to_filepath[item])\n",
    "        #image = self.transform(image=image)['image']\n",
    "        #image = self.to_tensor(image)\n",
    "        image = self.fastai_loader(self.datasetid_to_filepath[item])\n",
    "        label = self.datasetid_to_class_id[item]\n",
    "        \n",
    "        return image, label\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        returns size of dataset\n",
    "        '''\n",
    "        return len(self.df)\n",
    "    \n",
    "    def num_classes(self):\n",
    "        '''\n",
    "        returns number of classes\n",
    "        '''\n",
    "        return len(self.classes)\n",
    "    \n",
    "    def classes_value_counts(self):\n",
    "        '''\n",
    "        return number of samples per class\n",
    "        '''\n",
    "        return self.df.Label.value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import get_transforms, open_image, imagenet_stats\n",
    "fastai_transforms = get_transforms(flip_vert=True)\n",
    "def get_fastai_img(path, fastai_transforms\n",
    "                   , img_size=256):\n",
    "    '''\n",
    "    '''\n",
    "    fastai_img = open_image(path)\n",
    "    fastai_img = fastai_img.apply_tfms(*(fastai_transforms))\n",
    "    fastai_img = fastai_img.resize(img_size).px\n",
    "    fastai_img = transforms.Normalize(mean=imagenet_stats[0], std=imagenet_stats[1])(fastai_img)\n",
    "    \n",
    "    return fastai_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.beta import Beta\n",
    "def generate_mixup_samples(samples, labels, beta_distribution):\n",
    "    '''\n",
    "    generate mixup samples from 1 batch of samples, labels \n",
    "    and a pre-defined beta_distribution\n",
    "    '''\n",
    "    temp_batch_size = len(labels)\n",
    "    beta_weights = beta_distribution.sample((temp_batch_size,))\n",
    "    shuffled_index = torch.randperm(temp_batch_size)\n",
    "    samples_shuffled = samples[shuffled_index]\n",
    "    labels_shuffled = labels[shuffled_index]\n",
    "    \n",
    "    samples_mixup = torch.stack([beta_weights[i] * samples[i] \n",
    "                                 + (1 - beta_weights[i]) * samples_shuffled[i]\n",
    "                                 for i in range(temp_batch_size)])\n",
    "    labels_mixup = torch.stack([labels.float()\n",
    "                                , labels_shuffled.float()\n",
    "                                , beta_weights \n",
    "                               ])\n",
    "    return samples_mixup, labels_mixup\n",
    "\n",
    "class DataLoader_mixup(DataLoader):\n",
    "    '''\n",
    "    wrapper for DataLoader for mixup training:\n",
    "    for each batch, generate mixup samples\n",
    "    '''\n",
    "    def __init__(self, **kwargs\n",
    "                ):\n",
    "        super().__init__(**kwargs)\n",
    "        #self.dl = dl\n",
    "        self.beta_distribution = Beta(.4,.4)\n",
    "        \n",
    "    #def __len__(self):\n",
    "    #    return len(self)\n",
    "\n",
    "    def __iter__(self):\n",
    "        #batches = iter(self)\n",
    "        for samples, labels in self:\n",
    "            yield (generate_mixup_samples(samples, labels\n",
    "                                          , beta_distribution=self.beta_distribution)\n",
    "                  )\n",
    "\n",
    "class CrossEntropyLoss_mixup(nn.Module):\n",
    "    '''\n",
    "    wrapper of CrossEntropyLoss for mixup\n",
    "    using mixup while training and usual loss when evaluating\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, loss, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.loss_function = loss\n",
    "        self.reduction = reduction\n",
    "        setattr(self.loss_function, 'reduction', 'none')\n",
    "        \n",
    "    def forward(self, output, labels_mixup):\n",
    "        if len(labels_mixup.shape) != 1:\n",
    "            # mixup training mode\n",
    "            labels = labels_mixup[0,:].long()\n",
    "            labels_shuffled = labels_mixup[1,:].long()\n",
    "            beta_weights = labels_mixup[2,:]\n",
    "            \n",
    "            loss = (beta_weights * self.loss_function(output, labels)\n",
    "                   + (1-beta_weights) * self.loss_function(output, labels_shuffled)\n",
    "                   )\n",
    "            \n",
    "        else:\n",
    "            # evaluation mode, return loss as usual\n",
    "            loss = self.loss_function(output, labels_mixup)\n",
    "        \n",
    "        if self.reduction == 'mean': \n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll extract the classes that only have 1 sample here, they'll be excluded from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = pd.DataFrame(train_df.class_id.value_counts().reset_index()\n",
    "                           )\n",
    "label_counts.columns = ['class_id','count_samples']\n",
    "single_categories = label_counts[label_counts['count_samples'] < 2].class_id.values\n",
    "quarter_categories = label_counts[label_counts['count_samples'] > 4].class_id.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_sample_categories = label_counts[label_counts['count_samples'] > 200].class_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the Dataset class, we can then define the corresponding data loaders. The number of episodes is taken to be roughly the number of total classes divided by number of classes sampled for each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5002, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, val_subset = train_test_split(train_df[train_df.class_id.isin(quarter_categories)]\n",
    "                                 , test_size=.005 #originally .05\n",
    "                                )\n",
    "val_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just making sure that all classes in val set has at least 2 samples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_classes = val_subset.class_id.value_counts()[:8000].index.tolist()\n",
    "val_subset = val_subset[val_subset.class_id.isin(val_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(986384, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filter = ((train_df.class_id.isin(single_categories))\n",
    "               | (train_df.id.isin(val_subset.id))\n",
    "               )\n",
    "train_subset = train_df[-train_filter]\n",
    "train_subset\n",
    "train_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herbarium_nt=1_kt=30_qt=1_nv=1_kv=10_qv=1_seresnet101\n"
     ]
    }
   ],
   "source": [
    "evaluation_episodes = 50#1000\n",
    "episodes_per_epoch = 800\n",
    "\n",
    "if args.dataset == 'omniglot':\n",
    "    n_epochs = 40\n",
    "    dataset_class = OmniglotDataset\n",
    "    num_input_channels = 1\n",
    "    drop_lr_every = 20\n",
    "elif args.dataset == 'miniImageNet':\n",
    "    n_epochs = 35 #120\n",
    "    dataset_class = MiniImageNet\n",
    "    num_input_channels = 3\n",
    "    drop_lr_every = 40\n",
    "else:\n",
    "    n_epochs = 48 #120\n",
    "    #dataset_class = MiniImageNet\n",
    "    num_input_channels = 3\n",
    "    drop_lr_every = 24\n",
    "    #raise(ValueError, 'Unsupported dataset')\n",
    "\n",
    "param_str = f'{args.dataset}_nt={args.n_train}_kt={args.k_train}_qt={args.q_train}_' \\\n",
    "            f'nv={args.n_test}_kv={args.k_test}_qv={args.q_test}_seresnet101'\n",
    "\n",
    "print(param_str)\n",
    "\n",
    "###################\n",
    "# Create datasets #\n",
    "###################\n",
    "#background = dataset_class('background')\n",
    "background = myHerbariumDataset(train_subset\n",
    "                                , train=True\n",
    "                                , base_folder=train_dir \n",
    "                               )\n",
    "background_taskloader = DataLoader(\n",
    "    background,\n",
    "    batch_sampler=NShotTaskSampler(background, episodes_per_epoch, args.n_train, args.k_train, args.q_train),\n",
    "    num_workers=0\n",
    ")\n",
    "#background_taskloader = DataLoader_mixup(background_taskloader, Beta(.4,.4))\n",
    "evaluation = myHerbariumDataset(val_subset#.reset_index(drop=True)\n",
    "                                , train=True\n",
    "                                , base_folder=train_dir \n",
    "                               )\n",
    "evaluation_taskloader = DataLoader(\n",
    "    evaluation,\n",
    "    batch_sampler=NShotTaskSampler(evaluation, episodes_per_epoch, args.n_test, args.k_test, args.q_test),\n",
    "    num_workers=0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we use the model architecture predefined as in few_shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.layers import AdaptiveConcatPool2d, Flatten\n",
    "from pretrainedmodels import se_resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResnetProtoTypeNet():\n",
    "    \n",
    "    def my_head(input_size, hidden_units, output_size):\n",
    "        return nn.Sequential(AdaptiveConcatPool2d()\n",
    "                                        , Flatten()\n",
    "                                        , nn.BatchNorm1d(num_features=2 * input_size)\n",
    "                                        , nn.Dropout(p=.25)\n",
    "                                        , nn.Linear(in_features=2 * input_size, out_features=hidden_units, bias=True)\n",
    "                                        , nn.ReLU(inplace=True)\n",
    "                                        , nn.BatchNorm1d(num_features=hidden_units)\n",
    "                                        , nn.Dropout(p=.5)\n",
    "                                        , nn.Linear(in_features=hidden_units, out_features=output_size, bias=True)\n",
    "                                        \n",
    "                                       )\n",
    "\n",
    "    #arch = torchvision.models.resnet101(pretrained=False)\n",
    "    #arch = se_resnet101(pretrained=None)\n",
    "    arch = list(arch.children())\n",
    "    arch.pop(-1)\n",
    "    arch.pop(-1)\n",
    "    temp_arch = nn.Sequential(nn.Sequential(*arch))\n",
    "    temp_children = list(temp_arch.children())\n",
    "    temp_children.append(my_head(2048, 512, 200))\n",
    "    model = nn.Sequential(*temp_children)\n",
    "    \n",
    "    model_dir = '/bigdata/user/hieunt124/kaggle/herbarium/nybg2020/train/models/'\n",
    "    model_file = 'herbarium-seresnet101-weights.pth'\n",
    "    weights = torch.load(model_dir + model_file)\n",
    "\n",
    "    model.load_state_dict(weights['state_dict'])\n",
    "    \n",
    "    temp_head = list(model.children())[-1]\n",
    "    # temp_head = nn.Sequential(*list(temp_head.children())[:-2])\n",
    "    temp_head = nn.Sequential(*list(temp_head.children())[:2])\n",
    "    temp_arch = nn.Sequential(nn.Sequential(*list(model.children())[:-1]))\n",
    "    model = nn.Sequential(temp_arch, temp_head)\n",
    "    \n",
    "    return model\n",
    "    #return temp_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (3): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (3): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (4): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (5): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (7): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (8): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (9): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (10): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (11): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (12): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (13): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (14): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (15): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (16): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (17): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (18): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (19): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (20): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (21): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (22): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): AdaptiveConcatPool2d(\n",
       "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "    )\n",
       "    (1): Flatten()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#########\n",
    "# Model #\n",
    "#########\n",
    "#model = get_few_shot_encoder(num_input_channels)\n",
    "#model.to(device, dtype=torch.double)\n",
    "model = ResnetProtoTypeNet()\n",
    "model.to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ADAM optimizer and Negative log-likelihood loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, param in enumerate(list(model.parameters())[:-9]):\n",
    "    #print(i)\n",
    "    #param.require_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Prototypical network on Herbarium...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############\n",
    "# Training #\n",
    "############\n",
    "print(f'Training Prototypical network on {args.dataset}...')\n",
    "optimiser = Adam(model.parameters(), lr=1e-4)\n",
    "#loss_fn = torch.nn.NLLLoss().cuda()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#loss_fn = CrossEntropyLoss_mixup(loss_fn)\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    # Drop lr every 2000 episodes\n",
    "    if epoch % drop_lr_every == 0:\n",
    "        return lr * 0.6\n",
    "    else:\n",
    "        return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EvaluateFewShot(\n",
    "        eval_fn=proto_net_episode,\n",
    "        num_tasks=evaluation_episodes,\n",
    "        n_shot=args.n_test,\n",
    "        k_way=args.k_test,\n",
    "        q_queries=args.q_test,\n",
    "        taskloader=evaluation_taskloader,\n",
    "        prepare_batch=prepare_nshot_task(args.n_test, args.k_test, args.q_test),\n",
    "        distance=args.distance\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=PATH + f'/models/proto_nets/{param_str}.pth',\n",
    "        monitor=f'val_{args.n_test}-shot_{args.k_test}-way_acc'\n",
    "    ),\n",
    "    LearningRateScheduler(schedule=lr_schedule),\n",
    "    CSVLogger(PATH + f'/logs/proto_nets/{param_str}.csv'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1:   0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [2:17:57<00:00, 10.35s/it, loss=222, categorical_accuracy=0.7]     \n",
      "Epoch 2: 100%|██████████| 800/800 [1:55:59<00:00,  8.70s/it, loss=10.3, categorical_accuracy=0.525] \n",
      "Epoch 3: 100%|██████████| 800/800 [1:59:52<00:00,  8.99s/it, loss=2.16, categorical_accuracy=0.457] \n",
      "Epoch 4: 100%|██████████| 800/800 [2:11:30<00:00,  9.86s/it, loss=1.61, categorical_accuracy=0.527] \n",
      "Epoch 5: 100%|██████████| 800/800 [2:12:35<00:00,  9.94s/it, loss=1.38, categorical_accuracy=0.585] \n",
      "Epoch 6: 100%|██████████| 800/800 [2:03:14<00:00,  9.24s/it, loss=1.21, categorical_accuracy=0.631] \n",
      "Epoch 7: 100%|██████████| 800/800 [1:53:59<00:00,  8.55s/it, loss=1.1, categorical_accuracy=0.663]  \n",
      "Epoch 8: 100%|██████████| 800/800 [2:06:28<00:00,  9.49s/it, loss=1.03, categorical_accuracy=0.687]   \n",
      "Epoch 9: 100%|██████████| 800/800 [2:04:36<00:00,  9.35s/it, loss=0.966, categorical_accuracy=0.709] \n",
      "Epoch 10: 100%|██████████| 800/800 [2:12:18<00:00,  9.92s/it, loss=0.9, categorical_accuracy=0.726]    \n",
      "Epoch 11: 100%|██████████| 800/800 [1:58:52<00:00,  8.92s/it, loss=0.842, categorical_accuracy=0.746] \n",
      "Epoch 12: 100%|██████████| 800/800 [2:03:46<00:00,  9.28s/it, loss=0.842, categorical_accuracy=0.749] \n",
      "Epoch 13: 100%|██████████| 800/800 [2:00:43<00:00,  9.05s/it, loss=0.785, categorical_accuracy=0.761] \n",
      "Epoch 14: 100%|██████████| 800/800 [2:02:22<00:00,  9.18s/it, loss=0.74, categorical_accuracy=0.774]  \n",
      "Epoch 15: 100%|██████████| 800/800 [1:59:06<00:00,  8.93s/it, loss=0.731, categorical_accuracy=0.778] \n",
      "Epoch 16: 100%|██████████| 800/800 [2:00:15<00:00,  9.02s/it, loss=0.711, categorical_accuracy=0.785] \n",
      "Epoch 17: 100%|██████████| 800/800 [2:01:46<00:00,  9.13s/it, loss=0.717, categorical_accuracy=0.781] \n",
      "Epoch 18: 100%|██████████| 800/800 [1:59:42<00:00,  8.98s/it, loss=0.701, categorical_accuracy=0.785] \n",
      "Epoch 19: 100%|██████████| 800/800 [1:56:13<00:00,  8.72s/it, loss=0.678, categorical_accuracy=0.794] \n",
      "Epoch 20: 100%|██████████| 800/800 [1:56:37<00:00,  8.75s/it, loss=0.68, categorical_accuracy=0.792]  \n",
      "Epoch 21: 100%|██████████| 800/800 [1:55:38<00:00,  8.67s/it, loss=0.647, categorical_accuracy=0.801] \n",
      "Epoch 22: 100%|██████████| 800/800 [1:53:03<00:00,  8.48s/it, loss=0.657, categorical_accuracy=0.8]   \n",
      "Epoch 23: 100%|██████████| 800/800 [1:51:40<00:00,  8.38s/it, loss=0.632, categorical_accuracy=0.808] \n",
      "Epoch 24: 100%|██████████| 800/800 [1:53:49<00:00,  8.54s/it, loss=0.566, categorical_accuracy=0.825] \n",
      "Epoch 25: 100%|██████████| 800/800 [1:52:45<00:00,  8.46s/it, loss=0.528, categorical_accuracy=0.834] \n",
      "Epoch 26: 100%|██████████| 800/800 [1:49:28<00:00,  8.21s/it, loss=0.508, categorical_accuracy=0.842] \n",
      "Epoch 27: 100%|██████████| 800/800 [1:49:49<00:00,  8.24s/it, loss=0.511, categorical_accuracy=0.841] \n",
      "Epoch 28: 100%|██████████| 800/800 [1:47:41<00:00,  8.08s/it, loss=0.489, categorical_accuracy=0.849] \n",
      "Epoch 29: 100%|██████████| 800/800 [1:46:40<00:00,  8.00s/it, loss=0.485, categorical_accuracy=0.85]  \n",
      "Epoch 30: 100%|██████████| 800/800 [1:49:27<00:00,  8.21s/it, loss=0.47, categorical_accuracy=0.851]  \n",
      "Epoch 31: 100%|██████████| 800/800 [1:48:04<00:00,  8.11s/it, loss=0.459, categorical_accuracy=0.856] \n",
      "Epoch 32: 100%|██████████| 800/800 [1:46:17<00:00,  7.97s/it, loss=0.462, categorical_accuracy=0.854] \n",
      "Epoch 33: 100%|██████████| 800/800 [1:44:31<00:00,  7.84s/it, loss=0.45, categorical_accuracy=0.859]  \n",
      "Epoch 34: 100%|██████████| 800/800 [2:00:43<00:00,  9.05s/it, loss=0.454, categorical_accuracy=0.857] \n",
      "Epoch 35: 100%|██████████| 800/800 [1:50:10<00:00,  8.26s/it, loss=0.441, categorical_accuracy=0.865] \n",
      "Epoch 36: 100%|██████████| 800/800 [1:53:15<00:00,  8.49s/it, loss=0.443, categorical_accuracy=0.864] \n",
      "Epoch 37: 100%|██████████| 800/800 [2:00:35<00:00,  9.04s/it, loss=0.431, categorical_accuracy=0.865] \n",
      "Epoch 38: 100%|██████████| 800/800 [2:01:40<00:00,  9.13s/it, loss=0.417, categorical_accuracy=0.87]  \n",
      "Epoch 39: 100%|██████████| 800/800 [1:53:51<00:00,  8.54s/it, loss=0.424, categorical_accuracy=0.867] \n",
      "Epoch 40: 100%|██████████| 800/800 [1:59:10<00:00,  8.94s/it, loss=0.406, categorical_accuracy=0.871] \n",
      "Epoch 41: 100%|██████████| 800/800 [1:50:31<00:00,  8.29s/it, loss=0.417, categorical_accuracy=0.866] \n",
      "Epoch 42: 100%|██████████| 800/800 [1:51:02<00:00,  8.33s/it, loss=0.406, categorical_accuracy=0.872] \n",
      "Epoch 43: 100%|██████████| 800/800 [1:47:57<00:00,  8.10s/it, loss=0.414, categorical_accuracy=0.869] \n",
      "Epoch 44: 100%|██████████| 800/800 [1:52:45<00:00,  8.46s/it, loss=0.405, categorical_accuracy=0.872] \n",
      "Epoch 45: 100%|██████████| 800/800 [1:54:55<00:00,  8.62s/it, loss=0.394, categorical_accuracy=0.876] \n",
      "Epoch 46: 100%|██████████| 800/800 [2:06:45<00:00,  9.51s/it, loss=0.39, categorical_accuracy=0.877]  \n",
      "Epoch 47: 100%|██████████| 800/800 [2:03:41<00:00,  9.28s/it, loss=0.392, categorical_accuracy=0.879] \n",
      "Epoch 48:  76%|███████▋  | 612/800 [1:12:14<21:09,  6.75s/it, loss=0.447, categorical_accuracy=0.833] "
     ]
    }
   ],
   "source": [
    "\n",
    "fit(\n",
    "    model,\n",
    "    optimiser,\n",
    "    loss_fn,\n",
    "    epochs=n_epochs,\n",
    "    dataloader=background_taskloader,\n",
    "    prepare_batch=prepare_nshot_task(args.n_train, args.k_train, args.q_train),\n",
    "    callbacks=callbacks,\n",
    "    metrics=['categorical_accuracy'],\n",
    "    fit_function=proto_net_episode,\n",
    "    fit_function_kwargs={'n_shot': args.n_train, 'k_way': args.k_train, 'q_queries': args.q_train, 'train': True,\n",
    "                         'distance': args.distance}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained the model, let's try to test the model on the test set. For this, we'll create dataloaders for both train and test sets. We use the train dataloader to compute the prototypes for each class. We then determine the class of samples in the test set by taking the class with the minimal distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('/bigdata/user/hieunt124/submodules/few_shot/models/proto_nets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '/bigdata/user/hieunt124/submodules/few_shot/models/proto_nets/Herbarium_nt=1_kt=60_qt=1_nv=1_kv=10_qv=1_resnet_epoch30_cat790.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-create the dataset object, this time with all the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to jack up batchsize a bit if the GPU can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support_df = train_df.reset_index(drop=True)\n",
    "support_df = train_df[train_df.class_id.isin(np.arange(600))].reset_index(drop=True)\n",
    "support_dataset = myHerbariumDataset(support_df\n",
    "                                     , train=False\n",
    "                                     , base_folder=train_dir)\n",
    "support_loader = DataLoader(support_dataset, batch_size=256\n",
    "                            , shuffle=False\n",
    "                            , pin_memory=True\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = support_df.head(4096).copy()\n",
    "test_dataset = myHerbariumDataset(test_df\n",
    "                                  , train=False\n",
    "                                  , base_folder=train_dir\n",
    "                                 )\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False\n",
    "                         , num_workers=0\n",
    "                         , pin_memory=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class PrototypeSupportLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we loop through the dataloader, we save the embeddings and corresponding labels. The dataset is large so it's no quite viable to load all of the embeddings into a matrix, I'm thinking of how we can update the class prototype on the fly instead. So at any point, hopefully, we'll just need: 32k * 12k + 64 * 12k numbers, as opposed to 1m * 12k which is exhausting. We'll also need to keep track of how many samples have been used so far for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better yet, we can pre-sort the support dataset by class then write a custom dataloader which draws batch size according to how many samples there are for each class. This way, we won't have to compute loops within loops and can compute prototypes class by class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we'll define a Prototype class which will take as input the pretrained model, the annotations dataframe and the (custom) dataloader based on which it computes the class prototypes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating, this Prototype object takes as input the evaluation dataloader, then computes the embeddings for each sample in this val/test set. To save memory, for each sample, we'll save only the top-1 or top-5 least distances and the respective predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there's gonna be a shit ton of distances to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder if there's any merit in creating a ClassLoader and loop through that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prototypes():\n",
    "    def __init__(self, model, df, support_loader, device=torch.device('cuda')):\n",
    "        self.df = df\n",
    "        self.n_classes = self.df['class_id'].nunique()\n",
    "        self.support_loader = support_loader\n",
    "        self.model = model\n",
    "        self.classes = self.df['class_id'].unique().tolist()\n",
    "        self.device=device\n",
    "        self.model.to(device)\n",
    "        self._get_prototypes()\n",
    "        \n",
    "    \n",
    "    def _get_prototypes(self):\n",
    "        '''\n",
    "        compute class prototypes and store in self.class_prototypes \n",
    "        corresponding to class index\n",
    "        '''\n",
    "        #print('Computing prototypes...')\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (X, y) in (enumerate(self.support_loader)):\n",
    "                X, y = X.to(device, non_blocking=True), list(y)\n",
    "                X_embeddings = self.model.float()(X)#.cpu().detach().numpy()\n",
    "                #print(X_embeddings.shape[-1])\n",
    "                if batch_index == 0:\n",
    "                    \n",
    "                    # this matrix will hold the prototypes\n",
    "                    \n",
    "                    #self.class_prototypes = np.zeros((self.n_classes,X_embeddings.shape[-1]))\n",
    "                    self.class_prototypes = torch.zeros((self.n_classes,X_embeddings.shape[-1])).to(self.device)\n",
    "                                                 \n",
    "                    # this array will hold the item tally for each class, \n",
    "                    # this will also be updated on the fly\n",
    "                    #class_items_count = np.zeros(self.n_classes)\n",
    "                    class_items_count = torch.zeros(self.n_classes).to(self.device)\n",
    "                \n",
    "                for i, label in enumerate(y):\n",
    "                    \n",
    "                    label_index = self.classes.index(label)\n",
    "                    temp_item_count = class_items_count[label_index]\n",
    "                    #print(type(self.class_prototypes), type(label_index), type(X_embeddings), type(temp_item_count))\n",
    "                    self.class_prototypes[label_index] = (self.class_prototypes[label_index] * temp_item_count \n",
    "                                                    + X_embeddings[i]) / (temp_item_count+1)\n",
    "                    class_items_count[label_index]+=1\n",
    "                \n",
    "    def _get_embeddings(self, images):\n",
    "        '''\n",
    "        compute embeddings from input images\n",
    "        '''\n",
    "        #X_embeddings = self.model(images.float()).cpu().detach().numpy()\n",
    "        X_embeddings = self.model(images.float())\n",
    "        return X_embeddings\n",
    "    \n",
    "    def _get_predictions(self, embeddings\n",
    "                         , softmax=True\n",
    "                         , normalized_softmax=True\n",
    "                        ):\n",
    "        '''\n",
    "        compute pairwise distances from samples to each of the classes \n",
    "        and retain minimum distances to determine predicted classes\n",
    "        \n",
    "        '''\n",
    "        #preds = np.zeros((embeddings.shape[0], self.n_classes))\n",
    "        #for i, embedding in enumerate(embeddings):\n",
    "        \n",
    "        #for i, prototype in (enumerate(self.class_prototypes)):\n",
    "            #preds[:,i] = np.linalg.norm((embeddings - prototype), axis=1)\n",
    "             #preds[:,i] =                            \n",
    "        \n",
    "        preds = pairwise_euclidean_distance(embeddings, self.class_prototypes)\n",
    "        '''\n",
    "        if softmax:\n",
    "            if normalized_softmax:\n",
    "                preds /= np.max([1.0, np.abs(preds.mean())])\n",
    "            preds = np_softmax(preds)\n",
    "        '''\n",
    "        return preds\n",
    "    def predict(self, test_loader\n",
    "                , proba=True\n",
    "                , **kwargs):\n",
    "        '''\n",
    "        predict \n",
    "        '''\n",
    "        preds_list = []\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_index, (X_test, dummy_target) in tqdm(enumerate(test_loader)):\n",
    "                #time_elapsed = []\n",
    "                #time_elapsed.append(datetime.now())\n",
    "                X_test = X_test.to(device, non_blocking=True)\n",
    "                #time_elapsed.append(datetime.now())\n",
    "                X_embeddings = self._get_embeddings(X_test)\n",
    "                #time_elapsed.append(datetime.now())\n",
    "                temp_preds = self._get_predictions(X_embeddings)\n",
    "                #time_elapsed.append(datetime.now())\n",
    "                preds_list.append(temp_preds)\n",
    "                #time_elapsed.append(datetime.now())\n",
    "                #print(time_elapsed)\n",
    "        if (proba):\n",
    "            #return torch.cat(preds_list)\n",
    "            return np.concatenate(preds_list)\n",
    "        else:\n",
    "            \n",
    "            return ([self.classes[x] for x in torch.argmin(torch.cat(preds_list), axis=1)]\n",
    "                   , np.min(torch.cat(preds_list).cpu().detach().numpy(), axis=1)\n",
    "                   )\n",
    "            '''\n",
    "            return ([self.classes[x] for x in np.argmin(np.concatenate(preds_list), axis=1)]\n",
    "                    #np.argmin(np.concatenate(preds_list), axis=1)\n",
    "                    , np.min(np.concatenate(preds_list), axis=1)\n",
    "                   )\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_euclidean_distance(x, y):\n",
    "    '''\n",
    "    compute pairwise euclidean distance with torch tensors\n",
    "    '''\n",
    "    if not type(x) is torch.Tensor: x = torch.Tensor(x)\n",
    "    if not type(y) is torch.Tensor: y = torch.Tensor(y)\n",
    "        \n",
    "    n_x = x.shape[0]\n",
    "    n_y = y.shape[0]\n",
    "    distances = (\n",
    "                    x.cuda().unsqueeze(1).expand(n_x, n_y, -1) -\n",
    "                    y.cuda().unsqueeze(0).expand(n_x, n_y, -1)\n",
    "            ).pow(2).sum(dim=2)\n",
    "    return distances#.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meh = test_df.to_dict()['filepath']  ## get file path from image id\n",
    "#test_df.to_dict()['class_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%time herbarium_prototypes = Prototypes(model, support_df, support_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "130000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time predictions = herbarium_prototypes.predict(test_loader, proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(herbarium_prototypes.class_prototypes[:,0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suppose we can relieve some of memory load by computing prototypes on batches of classes at a time. For one batch of classes, we get a minimal distance and its corresponding class. By the end, we get say 50 candidates of (min distance, class) and just take the minimal of those. We won't be able to get top-5 accuracy this way but I'm struggling to find ways to lighten the memory load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_dataset.num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_class_batch = 50\n",
    "np_distance = np.zeros((test_df.shape[0], n_class_batch))\n",
    "np_class = np.zeros((test_df.shape[0], n_class_batch))\n",
    "batch_index = 0\n",
    "for class_batch in (np.array_split(np.arange(train_df.class_id.nunique()), n_class_batch)):\n",
    "    print(batch_index)\n",
    "    temp_support_df = train_df[train_df.class_id.isin(class_batch)].reset_index(drop=True)\n",
    "    support_dataset = myHerbariumDataset(temp_support_df\n",
    "                                         , train=False\n",
    "                                         , base_folder=train_dir)\n",
    "    support_loader = DataLoader(support_dataset, batch_size=256\n",
    "                                , shuffle=False\n",
    "                                , num_workers=0\n",
    "                               )\n",
    "    herbarium_prototypes = Prototypes(model, temp_support_df, support_loader)\n",
    "    temp_class, temp_distance = herbarium_prototypes.predict(test_loader=test_loader, proba=False)\n",
    "    np_class[:, batch_index] = temp_class\n",
    "    np_distance[:, batch_index] = temp_distance\n",
    "    batch_index+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_dir + metadata_file, encoding = \"ISO-8859-1\") as json_file:\n",
    "    test_metadata = json.load(json_file)\n",
    "\n",
    "test_df = pd.DataFrame(test_metadata['images'])\n",
    "test_df['class_id'] = 0\n",
    "test_df.rename(columns={'file_name': 'filepath'}, inplace=True)\n",
    "test_df.sort_values('id', inplace=True)\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.head(60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_df = train_df.reset_index(drop=True)\n",
    "support_dataset = myHerbariumDataset(support_df\n",
    "                                     , train=False\n",
    "                                     , base_folder=train_dir)\n",
    "support_loader = DataLoader(support_dataset, batch_size=256\n",
    "                            , shuffle=False\n",
    "                            , num_workers=2\n",
    "                           )\n",
    "test_dataset = myHerbariumDataset(test_df, train=False, base_folder=test_dir\n",
    "                                  \n",
    "                                 )\n",
    "test_loader = DataLoader(test_dataset, batch_size=128\n",
    "                         , shuffle=False\n",
    "                         , num_workers=0\n",
    "                         , pin_memory=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "herbarium_prototypes = Prototypes(model, support_df, support_loader)\n",
    "test_df['Predicted'] = herbarium_prototypes.predict(test_loader, proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.rename(columns={'id':'Id'}, inplace=True)\n",
    "test_df[['Id','Predicted']].to_csv(base_dir + 'submission_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle competitions submit -c herbarium-2020-fgvc7 -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh_preds = np.zeros((4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh_preds[:,0] = np.linalg.norm(meh, axis=1)\n",
    "meh_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(meh, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (X, y) in tqdm(enumerate(support_loader)):\n",
    "        X, y = X.to(device), list(y)\n",
    "        X_embeddings = model.float()(X).cpu().detach().numpy()\n",
    "        \n",
    "        if batch_index == 0:\n",
    "            # this matrix will hold the prototypes\n",
    "            class_prototypes = np.zeros((train_df.class_id.nunique()\n",
    "                                         ,X_embeddings.shape[-1]))\n",
    "            # this array will hold the item tally for each class, \n",
    "            # this will also be updated on the fly\n",
    "            class_items_count = np.zeros(train_df.class_id.nunique())\n",
    "        for i, label in enumerate(y):\n",
    "            temp_item_count = class_items_count[label]\n",
    "            class_prototypes[label] = (class_prototypes[label] * temp_item_count  + X_embeddings[i]) / (temp_item_count+1)\n",
    "            class_items_count[label]+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "#model.to('cpu')\n",
    "with torch.no_grad():\n",
    "    X_embeddings = model.float()(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embeddings.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably better to store a list of embeddings with corresponding list of classes, rather than storing a gigantic matrix of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh = np.random.randn(len(temp_loader.dataset), X_embeddings.shape[-1])\n",
    "meh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally produce a sample test prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stuff in tqdm((background_taskloader)):\n",
    "    image, label = stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh = background.df.to_dict()['filepath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[-train_df.class_id.isin(single_categories)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh[meh.id == 383760]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background.datasetid_to_filepath[383760]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background.loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.class_id == 6143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh = load_rgb_image(train_dir + train_df[train_df.id == 383760].filepath.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(meh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
