{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we perform classification of new images using prototypes networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate and store prototypes using a pretrained model. To classify new images, we use the image encoder to generate embeddings, then compute the pairwise distances from the embeddings to the prototypes. The prototype closest to the each embedding is chosen as the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['MKL_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                    # Array, Linear Algebra\n",
    "from torch.utils.data.dataset import random_split     # spliting inTrain Val\n",
    "import pandas as pd                                   # handling CSV\n",
    "import os                                             # For File handling\n",
    "import random                                         # Choosing from images dataset\n",
    "import time                                           # timing Epochs  \n",
    "from tqdm.notebook import tqdm                        # Testing\n",
    "from os.path import join                              # File Handling\n",
    "from torchvision import transforms                    # Data Aug\n",
    "import torch                                          # Framework\n",
    "from PIL import Image                                 # Loading Image\n",
    "from torch.utils.data import Dataset, DataLoader      # Dataset\n",
    "import torch.nn.functional as F                       # Function\n",
    "import json                                           # Loading Metadat\n",
    "from PIL import  ImageOps                             # Data Aug \n",
    "from PIL.Image import open as openIm                  # Image Handling\n",
    "import matplotlib.pyplot  as plt                      # Ploting Image\n",
    "import cv2\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the following line to import the functions from few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./few_shot/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Herbarium_nt=1_kt=15_qt=1_nv=1_kv=5_qv=1_toyresnet50.pth',\n",
       " 'Herbarium_nt=1_kt=30_qt=1_nv=1_kv=10_qv=1_seresnet101.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1_epoch=40.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1_resnet.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1_resnet_epoch20_cat725.pth',\n",
       " 'Herbarium_nt=1_kt=50_qt=1_nv=1_kv=10_qv=1.pth',\n",
       " 'Herbarium_nt=1_kt=60_qt=1_nv=1_kv=10_qv=1_resnet.pth',\n",
       " 'Herbarium_nt=1_kt=60_qt=1_nv=1_kv=10_qv=1_resnet_epoch30_cat790.pth',\n",
       " 'miniImageNet_nt=1_kt=50_qt=1_nv=1_kv=10_qv=1.pth',\n",
       " 'miniImageNet_nt=1_kt=60_qt=5_nv=1_kv=5_qv=1.pth',\n",
       " 'miniImageNet_nt=5_kt=20_qt=15_nv=5_kv=5_qv=1.pth',\n",
       " 'miniImageNet_nt=5_kt=60_qt=5_nv=1_kv=5_qv=1.pth',\n",
       " 'miniImageNet_nt=5_kt=60_qt=5_nv=5_kv=5_qv=5.pth']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./few_shot/models/proto_nets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reproduce Omniglot results of Snell et al Prototypical networks.\n",
    "\"\"\"\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "\n",
    "from few_shot.datasets import OmniglotDataset, MiniImageNet\n",
    "from few_shot.models import get_few_shot_encoder\n",
    "from few_shot.core import NShotTaskSampler, EvaluateFewShot, prepare_nshot_task\n",
    "from few_shot.proto import proto_net_episode\n",
    "from few_shot.train import fit\n",
    "from few_shot.callbacks import *\n",
    "from few_shot.utils import setup_dirs\n",
    "from config import PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import vision stuff\n",
    "'''\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add directories and load metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_dir = '/bigdata/user/hieunt124/kaggle/herbarium/'\n",
    "base_dir = 'D:/Data/Kaggle_HerbariumChallenge2020'\n",
    "train_dir = base_dir + '/nybg2020/train/'\n",
    "test_dir = base_dir + '/nybg2020/test/'\n",
    "metadata_file = 'metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76407</th>\n",
       "      <td>0</td>\n",
       "      <td>626762</td>\n",
       "      <td>images/000/00/626762.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601590</th>\n",
       "      <td>0</td>\n",
       "      <td>72077</td>\n",
       "      <td>images/000/00/72077.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76408</th>\n",
       "      <td>0</td>\n",
       "      <td>818271</td>\n",
       "      <td>images/000/00/818271.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556748</th>\n",
       "      <td>0</td>\n",
       "      <td>495523</td>\n",
       "      <td>images/000/00/495523.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335261</th>\n",
       "      <td>0</td>\n",
       "      <td>437000</td>\n",
       "      <td>images/000/00/437000.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category_id      id                 file_name  height  width\n",
       "76407             0  626762  images/000/00/626762.jpg    1000    681\n",
       "601590            0   72077   images/000/00/72077.jpg    1000    681\n",
       "76408             0  818271  images/000/00/818271.jpg    1000    681\n",
       "556748            0  495523  images/000/00/495523.jpg    1000    681\n",
       "335261            0  437000  images/000/00/437000.jpg    1000    681"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(train_dir + metadata_file, encoding = \"ISO-8859-1\") as json_file:\n",
    "    train_metadata = json.load(json_file)\n",
    "\n",
    "train_img = pd.DataFrame(train_metadata['images'])\n",
    "train_label = pd.DataFrame(train_metadata['annotations'])\n",
    "train_df = (pd.merge(train_label, train_img\n",
    "                    #, left_on='image_id'\n",
    "                    , on='id'\n",
    "                    , how='left')\n",
    "            .drop(['image_id', 'license', 'region_id'], axis=1)\n",
    "            .sort_values(by=['category_id'])\n",
    "           )\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.rename(columns={'category_id': 'class_id'\n",
    "                        , 'file_name': 'filepath'\n",
    "                        }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the training parameters. For now, we can keep them as is, just to check the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n##############\\n# Parameters #\\n##############\\nparser = argparse.ArgumentParser()\\nparser.add_argument('--dataset', default='miniImageNet')\\nparser.add_argument('--distance', default='l2')\\nparser.add_argument('--n-train', default=5, type=int)\\nparser.add_argument('--n-test', default=5, type=int)\\nparser.add_argument('--k-train', default=20, type=int)\\nparser.add_argument('--k-test', default=5, type=int)\\nparser.add_argument('--q-train', default=15, type=int)\\nparser.add_argument('--q-test', default=1, type=int)\\nargs = parser.parse_args('')\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_dirs()\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "'''\n",
    "\n",
    "##############\n",
    "# Parameters #\n",
    "##############\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='miniImageNet')\n",
    "parser.add_argument('--distance', default='l2')\n",
    "parser.add_argument('--n-train', default=5, type=int)\n",
    "parser.add_argument('--n-test', default=5, type=int)\n",
    "parser.add_argument('--k-train', default=20, type=int)\n",
    "parser.add_argument('--k-test', default=5, type=int)\n",
    "parser.add_argument('--q-train', default=15, type=int)\n",
    "parser.add_argument('--q-test', default=1, type=int)\n",
    "args = parser.parse_args('')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define dataset object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we'll omit the classes with only 1 sample, which from value_counts include 3 classes. The remaining classes contain at least 2 samples so we can still perform multi-way, 1-shot 1-query training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "def load_rgb_image(image_file):\n",
    "    '''\n",
    "    load image file in RGB format\n",
    "    '''\n",
    "    img = cv2.imread(str(image_file))\n",
    "    try:\n",
    "        #img = img.astype('uint8')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        print(image_file)\n",
    "        print(img.shape)\n",
    "    return img\n",
    "\n",
    "def get_augmentations(re_size=300#224\n",
    "                      , crop_size=300#224\n",
    "                      , train=True\n",
    "                     ):\n",
    "    '''\n",
    "    get image augmentations from albumentations\n",
    "    '''\n",
    "    augs = [A.Resize(height=re_size, width=re_size)]\n",
    "    if train:\n",
    "        augs.extend([A.RandomCrop(height=crop_size, width=crop_size)\n",
    "                     , A.ShiftScaleRotate(shift_limit=.1, scale_limit=.3, rotate_limit=30, p=.75)\n",
    "                     , A.RandomBrightnessContrast(brightness_limit=.5, contrast_limit=.5, p=.75)\n",
    "                     #, A.Blur(.5)\n",
    "                     , A.Cutout(max_h_size=crop_size//12, max_w_size=crop_size//12, p=.5)\n",
    "                    ])\n",
    "    else:\n",
    "        augs.extend([A.CenterCrop(height=crop_size, width=crop_size)])\n",
    "    \n",
    "    # A.Normalize uses Imagenet stats by default\n",
    "    return A.Compose(augs + [A.Normalize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myHerbariumDataset(Dataset\n",
    "                        ):\n",
    "    def __init__(self, df_image\n",
    "                         , train=True\n",
    "                         , aug=True\n",
    "                         , base_folder='../kaggle/herbarium/'):\n",
    "        if (train):\n",
    "            df_image.index = df_image['id']\n",
    "\n",
    "        self.df = df_image ## dataframe of all image annotations\n",
    "        self.datasetid_to_filepath = df_image.to_dict()['filepath']  ## get file path from image id\n",
    "        self.datasetid_to_class_id = df_image.to_dict()['class_id']  ## get class id from image id\n",
    "\n",
    "        self.classes = self.df['class_id'].unique() ## list of labels\n",
    "        self.base_folder = base_folder\n",
    "        self.loader = lambda x: load_rgb_image(base_folder + x)  ## loader function for the image\n",
    "        self.transform = get_augmentations(train=aug) ## transform the image\n",
    "        self.to_tensor = transforms.ToTensor()  ## transform image to Torch tensor\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        '''\n",
    "        input item id, output the image and its label\n",
    "        '''\n",
    "        \n",
    "        image = self.loader(self.datasetid_to_filepath[item])\n",
    "        image = self.transform(image=image)['image']\n",
    "        image = self.to_tensor(image)\n",
    "        label = self.datasetid_to_class_id[item]\n",
    "        \n",
    "        return image, label\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        returns size of dataset\n",
    "        '''\n",
    "        return len(self.df)\n",
    "    \n",
    "    def num_classes(self):\n",
    "        '''\n",
    "        returns number of classes\n",
    "        '''\n",
    "        return len(self.classes)\n",
    "    \n",
    "    def classes_value_counts(self):\n",
    "        '''\n",
    "        return number of samples per class\n",
    "        '''\n",
    "        return self.df.Label.value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adv_inception_v3',\n",
      " 'densenet121',\n",
      " 'densenet161',\n",
      " 'densenet169',\n",
      " 'densenet201',\n",
      " 'densenetblur121d',\n",
      " 'dla34',\n",
      " 'dla46_c',\n",
      " 'dla46x_c',\n",
      " 'dla60',\n",
      " 'dla60_res2net',\n",
      " 'dla60_res2next',\n",
      " 'dla60x',\n",
      " 'dla60x_c',\n",
      " 'dla102',\n",
      " 'dla102x',\n",
      " 'dla102x2',\n",
      " 'dla169',\n",
      " 'dpn68',\n",
      " 'dpn68b',\n",
      " 'dpn92',\n",
      " 'dpn98',\n",
      " 'dpn107',\n",
      " 'dpn131',\n",
      " 'ecaresnet50d',\n",
      " 'ecaresnet50d_pruned',\n",
      " 'ecaresnet101d',\n",
      " 'ecaresnet101d_pruned',\n",
      " 'ecaresnetlight',\n",
      " 'efficientnet_b0',\n",
      " 'efficientnet_b1',\n",
      " 'efficientnet_b1_pruned',\n",
      " 'efficientnet_b2',\n",
      " 'efficientnet_b2_pruned',\n",
      " 'efficientnet_b2a',\n",
      " 'efficientnet_b3',\n",
      " 'efficientnet_b3_pruned',\n",
      " 'efficientnet_b3a',\n",
      " 'efficientnet_es',\n",
      " 'ens_adv_inception_resnet_v2',\n",
      " 'ese_vovnet19b_dw',\n",
      " 'ese_vovnet39b',\n",
      " 'fbnetc_100',\n",
      " 'gluon_inception_v3',\n",
      " 'gluon_resnet18_v1b',\n",
      " 'gluon_resnet34_v1b',\n",
      " 'gluon_resnet50_v1b',\n",
      " 'gluon_resnet50_v1c',\n",
      " 'gluon_resnet50_v1d',\n",
      " 'gluon_resnet50_v1s',\n",
      " 'gluon_resnet101_v1b',\n",
      " 'gluon_resnet101_v1c',\n",
      " 'gluon_resnet101_v1d',\n",
      " 'gluon_resnet101_v1s',\n",
      " 'gluon_resnet152_v1b',\n",
      " 'gluon_resnet152_v1c',\n",
      " 'gluon_resnet152_v1d',\n",
      " 'gluon_resnet152_v1s',\n",
      " 'gluon_resnext50_32x4d',\n",
      " 'gluon_resnext101_32x4d',\n",
      " 'gluon_resnext101_64x4d',\n",
      " 'gluon_senet154',\n",
      " 'gluon_seresnext50_32x4d',\n",
      " 'gluon_seresnext101_32x4d',\n",
      " 'gluon_seresnext101_64x4d',\n",
      " 'gluon_xception65',\n",
      " 'hrnet_w18',\n",
      " 'hrnet_w18_small',\n",
      " 'hrnet_w18_small_v2',\n",
      " 'hrnet_w30',\n",
      " 'hrnet_w32',\n",
      " 'hrnet_w40',\n",
      " 'hrnet_w44',\n",
      " 'hrnet_w48',\n",
      " 'hrnet_w64',\n",
      " 'ig_resnext101_32x8d',\n",
      " 'ig_resnext101_32x16d',\n",
      " 'ig_resnext101_32x32d',\n",
      " 'ig_resnext101_32x48d',\n",
      " 'inception_resnet_v2',\n",
      " 'inception_v3',\n",
      " 'inception_v4',\n",
      " 'mixnet_l',\n",
      " 'mixnet_m',\n",
      " 'mixnet_s',\n",
      " 'mixnet_xl',\n",
      " 'mnasnet_100',\n",
      " 'mobilenetv2_100',\n",
      " 'mobilenetv2_110d',\n",
      " 'mobilenetv2_120d',\n",
      " 'mobilenetv2_140',\n",
      " 'mobilenetv3_large_100',\n",
      " 'mobilenetv3_rw',\n",
      " 'nasnetalarge',\n",
      " 'pnasnet5large',\n",
      " 'regnetx_002',\n",
      " 'regnetx_004',\n",
      " 'regnetx_006',\n",
      " 'regnetx_008',\n",
      " 'regnetx_016',\n",
      " 'regnetx_032',\n",
      " 'regnetx_040',\n",
      " 'regnetx_064',\n",
      " 'regnetx_080',\n",
      " 'regnetx_120',\n",
      " 'regnetx_160',\n",
      " 'regnetx_320',\n",
      " 'regnety_002',\n",
      " 'regnety_004',\n",
      " 'regnety_006',\n",
      " 'regnety_008',\n",
      " 'regnety_016',\n",
      " 'regnety_032',\n",
      " 'regnety_040',\n",
      " 'regnety_064',\n",
      " 'regnety_080',\n",
      " 'regnety_120',\n",
      " 'regnety_160',\n",
      " 'regnety_320',\n",
      " 'res2net50_14w_8s',\n",
      " 'res2net50_26w_4s',\n",
      " 'res2net50_26w_6s',\n",
      " 'res2net50_26w_8s',\n",
      " 'res2net50_48w_2s',\n",
      " 'res2net101_26w_4s',\n",
      " 'res2next50',\n",
      " 'resnest14d',\n",
      " 'resnest26d',\n",
      " 'resnest50d',\n",
      " 'resnest50d_1s4x24d',\n",
      " 'resnest50d_4s2x40d',\n",
      " 'resnest101e',\n",
      " 'resnest200e',\n",
      " 'resnest269e',\n",
      " 'resnet18',\n",
      " 'resnet26',\n",
      " 'resnet26d',\n",
      " 'resnet34',\n",
      " 'resnet50',\n",
      " 'resnet101',\n",
      " 'resnet152',\n",
      " 'resnetblur50',\n",
      " 'resnext50_32x4d',\n",
      " 'resnext50d_32x4d',\n",
      " 'resnext101_32x8d',\n",
      " 'selecsls42b',\n",
      " 'selecsls60',\n",
      " 'selecsls60b',\n",
      " 'semnasnet_100',\n",
      " 'senet154',\n",
      " 'seresnet18',\n",
      " 'seresnet34',\n",
      " 'seresnet50',\n",
      " 'seresnet101',\n",
      " 'seresnet152',\n",
      " 'seresnext26_32x4d',\n",
      " 'seresnext26d_32x4d',\n",
      " 'seresnext26t_32x4d',\n",
      " 'seresnext26tn_32x4d',\n",
      " 'seresnext50_32x4d',\n",
      " 'seresnext101_32x4d',\n",
      " 'skresnet18',\n",
      " 'skresnet34',\n",
      " 'skresnext50_32x4d',\n",
      " 'spnasnet_100',\n",
      " 'ssl_resnet18',\n",
      " 'ssl_resnet50',\n",
      " 'ssl_resnext50_32x4d',\n",
      " 'ssl_resnext101_32x4d',\n",
      " 'ssl_resnext101_32x8d',\n",
      " 'ssl_resnext101_32x16d',\n",
      " 'swsl_resnet18',\n",
      " 'swsl_resnet50',\n",
      " 'swsl_resnext50_32x4d',\n",
      " 'swsl_resnext101_32x4d',\n",
      " 'swsl_resnext101_32x8d',\n",
      " 'swsl_resnext101_32x16d',\n",
      " 'tf_efficientnet_b0',\n",
      " 'tf_efficientnet_b0_ap',\n",
      " 'tf_efficientnet_b0_ns',\n",
      " 'tf_efficientnet_b1',\n",
      " 'tf_efficientnet_b1_ap',\n",
      " 'tf_efficientnet_b1_ns',\n",
      " 'tf_efficientnet_b2',\n",
      " 'tf_efficientnet_b2_ap',\n",
      " 'tf_efficientnet_b2_ns',\n",
      " 'tf_efficientnet_b3',\n",
      " 'tf_efficientnet_b3_ap',\n",
      " 'tf_efficientnet_b3_ns',\n",
      " 'tf_efficientnet_b4',\n",
      " 'tf_efficientnet_b4_ap',\n",
      " 'tf_efficientnet_b4_ns',\n",
      " 'tf_efficientnet_b5',\n",
      " 'tf_efficientnet_b5_ap',\n",
      " 'tf_efficientnet_b5_ns',\n",
      " 'tf_efficientnet_b6',\n",
      " 'tf_efficientnet_b6_ap',\n",
      " 'tf_efficientnet_b6_ns',\n",
      " 'tf_efficientnet_b7',\n",
      " 'tf_efficientnet_b7_ap',\n",
      " 'tf_efficientnet_b7_ns',\n",
      " 'tf_efficientnet_b8',\n",
      " 'tf_efficientnet_b8_ap',\n",
      " 'tf_efficientnet_cc_b0_4e',\n",
      " 'tf_efficientnet_cc_b0_8e',\n",
      " 'tf_efficientnet_cc_b1_8e',\n",
      " 'tf_efficientnet_el',\n",
      " 'tf_efficientnet_em',\n",
      " 'tf_efficientnet_es',\n",
      " 'tf_efficientnet_l2_ns',\n",
      " 'tf_efficientnet_l2_ns_475',\n",
      " 'tf_efficientnet_lite0',\n",
      " 'tf_efficientnet_lite1',\n",
      " 'tf_efficientnet_lite2',\n",
      " 'tf_efficientnet_lite3',\n",
      " 'tf_efficientnet_lite4',\n",
      " 'tf_inception_v3',\n",
      " 'tf_mixnet_l',\n",
      " 'tf_mixnet_m',\n",
      " 'tf_mixnet_s',\n",
      " 'tf_mobilenetv3_large_075',\n",
      " 'tf_mobilenetv3_large_100',\n",
      " 'tf_mobilenetv3_large_minimal_100',\n",
      " 'tf_mobilenetv3_small_075',\n",
      " 'tf_mobilenetv3_small_100',\n",
      " 'tf_mobilenetv3_small_minimal_100',\n",
      " 'tresnet_l',\n",
      " 'tresnet_l_448',\n",
      " 'tresnet_m',\n",
      " 'tresnet_m_448',\n",
      " 'tresnet_xl',\n",
      " 'tresnet_xl_448',\n",
      " 'tv_densenet121',\n",
      " 'tv_resnet34',\n",
      " 'tv_resnet50',\n",
      " 'tv_resnext50_32x4d',\n",
      " 'wide_resnet50_2',\n",
      " 'wide_resnet101_2',\n",
      " 'xception']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pretrained_models = timm.list_models(pretrained=True)\n",
    "pprint(pretrained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.layers import AdaptiveConcatPool2d, Flatten\n",
    "\n",
    "def ResnetProtoTypeNet():\n",
    "    \n",
    "    def my_head(input_size, hidden_units, output_size):\n",
    "        return nn.Sequential(AdaptiveConcatPool2d()\n",
    "                                        , Flatten()\n",
    "                                        , nn.BatchNorm1d(num_features=2 * input_size)\n",
    "                                        , nn.Dropout(p=.25)\n",
    "                                        , nn.Linear(in_features=2 * input_size, out_features=hidden_units, bias=True)\n",
    "                                        , nn.ReLU(inplace=True)\n",
    "                                        , nn.BatchNorm1d(num_features=hidden_units)\n",
    "                                        , nn.Dropout(p=.5)\n",
    "                                        , nn.Linear(in_features=hidden_units, out_features=output_size, bias=True)\n",
    "                                        \n",
    "                                       )\n",
    "\n",
    "    #arch = se_resnet101(pretrained=None)\n",
    "    arch = timm.create_model('seresnet101', pretrained=False)\n",
    "    arch = list(arch.children())\n",
    "    arch.pop(-1)\n",
    "    arch.pop(-1)\n",
    "    temp_arch = nn.Sequential(nn.Sequential(*arch))\n",
    "    temp_children = list(temp_arch.children())\n",
    "    temp_children.append(my_head(2048, 512, 200))\n",
    "    model = nn.Sequential(*temp_children)\n",
    "    \n",
    "    #model_dir = '/bigdata/user/hieunt124/kaggle/herbarium/nybg2020/train/models/'\n",
    "    #model_file = 'herbarium-seresnet101-weights.pth'\n",
    "    #weights = torch.load(model_dir + model_file)\n",
    "\n",
    "    #model.load_state_dict(weights['state_dict'])\n",
    "\n",
    "    temp_head = list(model.children())[-1]\n",
    "    # temp_head = nn.Sequential(*list(temp_head.children())[:-2])\n",
    "    temp_head = nn.Sequential(*list(temp_head.children())[:2])\n",
    "    temp_arch = nn.Sequential(nn.Sequential(*list(model.children())[:-1]))\n",
    "    model = nn.Sequential(temp_arch, temp_head)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (3): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (3): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (4): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (5): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (7): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (8): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (9): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (10): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (11): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (12): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (13): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (14): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (15): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (16): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (17): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (18): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (19): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (20): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (21): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (22): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (2): SEResNetBottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (se_module): SEModule(\n",
       "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): AdaptiveConcatPool2d(\n",
       "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "    )\n",
       "    (1): Flatten()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResnetProtoTypeNet()\n",
    "model.to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained the model, let's try to test the model on the test set. For this, we'll create dataloaders for both train and test sets. We use the train dataloader to compute the prototypes for each class. We then determine the class of samples in the test set by taking the class with the minimal distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Herbarium_nt=1_kt=15_qt=1_nv=1_kv=5_qv=1_toyresnet50.pth',\n",
       " 'Herbarium_nt=1_kt=30_qt=1_nv=1_kv=10_qv=1_seresnet101.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1_epoch=40.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1_resnet.pth',\n",
       " 'Herbarium_nt=1_kt=40_qt=1_nv=1_kv=10_qv=1_resnet_epoch20_cat725.pth',\n",
       " 'Herbarium_nt=1_kt=50_qt=1_nv=1_kv=10_qv=1.pth',\n",
       " 'Herbarium_nt=1_kt=60_qt=1_nv=1_kv=10_qv=1_resnet.pth',\n",
       " 'Herbarium_nt=1_kt=60_qt=1_nv=1_kv=10_qv=1_resnet_epoch30_cat790.pth',\n",
       " 'miniImageNet_nt=1_kt=50_qt=1_nv=1_kv=10_qv=1.pth',\n",
       " 'miniImageNet_nt=1_kt=60_qt=5_nv=1_kv=5_qv=1.pth',\n",
       " 'miniImageNet_nt=5_kt=20_qt=15_nv=5_kv=5_qv=1.pth',\n",
       " 'miniImageNet_nt=5_kt=60_qt=5_nv=1_kv=5_qv=1.pth',\n",
       " 'miniImageNet_nt=5_kt=60_qt=5_nv=5_kv=5_qv=5.pth']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./few_shot/models/proto_nets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = './few_shot/models/proto_nets/Herbarium_nt=1_kt=30_qt=1_nv=1_kv=10_qv=1_seresnet101.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a slight note here that\n",
    "# apparently, when loading the weights, there's also info on the cuda settings\n",
    "# which might cause some consistency error\n",
    "# to deal with, simply include map_location='<stuff>' with torch.load()\n",
    "model.load_state_dict(torch.load(model_file, map_location='cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to jack up batchsize a bit if the GPU can handle it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some notes on code choices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original competition, there were roughly 140k images to classify with 32k labels using a train set of 1m images. With prototype networks, that means storing a matrix of prototypes of size 32k x 512 where 512 is the size of the embedding vector. The prediction is done by computing pairwise distances from the embedding vectors of test images to the prototypes and then argmin-ing the class where the distance is the smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to compute the matrix of prototypes. This is done by computing, for each class, the samples embedding vectors and then taking their average. As the train set is large, the classes prototypes are updated by batches instead of being calculated in one go.  \n",
    "I should note that in the original comment, I wrote that \"we'll just need: 32k * 12k + 64 * 12k numbers, as opposed to 1m * 12k which is exhausting\" .I'm not sure how I came to the number 12k, I hope it was just a mistake but I include it here anyway just in case something comes up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we'll define a Prototype class which will take as input the pretrained model, the annotations dataframe and the (custom) dataloader based on which it computes the class prototypes. Once the prototypes have been computed and stored, this Prototype object takes in test dataloaders to classify by computing the distance matrix and output the row-wise argmin as class.\n",
    "\n",
    "Code-wise, the class starts with an __init()__ function where it stores the DataLoader for the support set, the dataframe to get the labels from and the model to get the embeddings from. The class then computes and stores prototypes via _get_prototypes()_. For classification, the class calls the __predict(test_loader)__ function. For each batch in the test dataloader, the class get the embeddings for the images in the batch via _get_embeddings()_, then compute the distance matrix via _get_predictions()_. Finally, we return either the full distance matrix (for proba=True) or the pairs (predicted class, smallest distance).\n",
    "\n",
    "We use the latter option for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prototypes():\n",
    "    def __init__(self, model, df, support_loader, device=torch.device('cuda')):\n",
    "        self.df = df\n",
    "        self.n_classes = self.df['class_id'].nunique()\n",
    "        self.support_loader = support_loader\n",
    "        self.model = model\n",
    "        self.classes = self.df['class_id'].unique().tolist()\n",
    "        self.device=device\n",
    "        self.model.to(device)\n",
    "        self._get_prototypes()\n",
    "        \n",
    "    \n",
    "    def _get_prototypes(self):\n",
    "        '''\n",
    "        compute class prototypes and store in self.class_prototypes \n",
    "        corresponding to class index\n",
    "        '''\n",
    "        print('Computing prototypes...')\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (X, y) in (enumerate(self.support_loader)):\n",
    "                X, y = X.to(device, non_blocking=True), list(y)\n",
    "                X_embeddings = self.model.float()(X)#.cpu().detach().numpy()\n",
    "                #print(X_embeddings.shape[-1])\n",
    "                if batch_index == 0:\n",
    "                    \n",
    "                    # this matrix will hold the prototypes\n",
    "                    \n",
    "                    #self.class_prototypes = np.zeros((self.n_classes,X_embeddings.shape[-1]))\n",
    "                    self.class_prototypes = torch.zeros((self.n_classes,X_embeddings.shape[-1])).to(self.device)\n",
    "                                                 \n",
    "                    # this array will hold the item tally for each class, \n",
    "                    # this will also be updated on the fly\n",
    "                    #class_items_count = np.zeros(self.n_classes)\n",
    "                    class_items_count = torch.zeros(self.n_classes).to(self.device)\n",
    "                \n",
    "                for i, label in enumerate(y):\n",
    "                    \n",
    "                    label_index = self.classes.index(label)\n",
    "                    temp_item_count = class_items_count[label_index]\n",
    "                    #print(type(self.class_prototypes), type(label_index), type(X_embeddings), type(temp_item_count))\n",
    "                    self.class_prototypes[label_index] = (self.class_prototypes[label_index] * temp_item_count \n",
    "                                                    + X_embeddings[i]) / (temp_item_count+1)\n",
    "                    class_items_count[label_index]+=1\n",
    "                \n",
    "    def _get_embeddings(self, images):\n",
    "        '''\n",
    "        compute embeddings from input images\n",
    "        '''\n",
    "        #X_embeddings = self.model(images.float()).cpu().detach().numpy()\n",
    "        X_embeddings = self.model(images.float())\n",
    "        return X_embeddings\n",
    "    \n",
    "    def _get_predictions(self, embeddings\n",
    "                         , softmax=True\n",
    "                         , normalized_softmax=True\n",
    "                        ):\n",
    "        '''\n",
    "        compute pairwise distances from samples to each of the classes \n",
    "        and retain minimum distances to determine predicted classes\n",
    "        \n",
    "        '''\n",
    "        preds = pairwise_euclidean_distance(embeddings, self.class_prototypes)\n",
    "        '''\n",
    "        if softmax:\n",
    "            if normalized_softmax:\n",
    "                preds /= np.max([1.0, np.abs(preds.mean())])\n",
    "            preds = np_softmax(preds)\n",
    "        '''\n",
    "        return preds\n",
    "    def predict(self, test_loader\n",
    "                , proba=True\n",
    "                , **kwargs):\n",
    "        '''\n",
    "        predict \n",
    "        '''\n",
    "        preds_list = []\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_index, (X_test, dummy_target) in tqdm(enumerate(test_loader)):\n",
    "                X_test = X_test.to(device, non_blocking=True)\n",
    "                X_embeddings = self._get_embeddings(X_test)\n",
    "                temp_preds = self._get_predictions(X_embeddings)\n",
    "                preds_list.append(temp_preds)\n",
    "                \n",
    "        if (proba):\n",
    "            return np.concatenate(preds_list)\n",
    "        else:\n",
    "            \n",
    "            return ([self.classes[x] for x in torch.argmin(torch.cat(preds_list), axis=1)]\n",
    "                   , np.min(torch.cat(preds_list).cpu().detach().numpy(), axis=1)\n",
    "                   )\n",
    "            '''\n",
    "            return ([self.classes[x] for x in np.argmin(np.concatenate(preds_list), axis=1)]\n",
    "                    #np.argmin(np.concatenate(preds_list), axis=1)\n",
    "                    , np.min(np.concatenate(preds_list), axis=1)\n",
    "                   )\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_euclidean_distance(x, y):\n",
    "    '''\n",
    "    compute pairwise euclidean distance with torch tensors\n",
    "    '''\n",
    "    if not type(x) is torch.Tensor: x = torch.Tensor(x)\n",
    "    if not type(y) is torch.Tensor: y = torch.Tensor(y)\n",
    "        \n",
    "    n_x = x.shape[0]\n",
    "    n_y = y.shape[0]\n",
    "    distances = (\n",
    "                    x.cuda().unsqueeze(1).expand(n_x, n_y, -1) -\n",
    "                    y.cuda().unsqueeze(0).expand(n_x, n_y, -1)\n",
    "            ).pow(2).sum(dim=2)\n",
    "    return distances#.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo predicting with prototypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we perform prototypes computation and classification on some toy support-test sets. The support data is sampled from the train set, taking the first 400 classes. The test subset is then sampled from this support set.\n",
    "\n",
    "This is what the code would've looked like had the data been small enough to compute prototypes and distances all in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_df = train_df[train_df.class_id.isin(np.arange(400))].reset_index(drop=True)\n",
    "support_dataset = myHerbariumDataset(support_df\n",
    "                                     , train=False\n",
    "                                     , aug=False\n",
    "                                     , base_folder=train_dir)\n",
    "support_loader = DataLoader(support_dataset, batch_size=32\n",
    "                            , shuffle=False\n",
    "                            , pin_memory=True\n",
    "                           )\n",
    "test_df = support_df.sample(1024).copy().reset_index(drop=True)\n",
    "#test_df = (train_df[train_df.class_id.isin(sample_classes)]\n",
    "#           .sample(1024).copy()\n",
    "#           .reset_index(drop=True)\n",
    "#         )\n",
    "test_dataset = myHerbariumDataset(test_df\n",
    "                                  , train=False\n",
    "                                  , aug=False\n",
    "                                  , base_folder=train_dir\n",
    "                                 )\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False\n",
    "                         , num_workers=0\n",
    "                         , pin_memory=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prototypes can be computed like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing prototypes...\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%time herbarium_prototypes = Prototypes(model, support_df, support_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the prediction like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:13,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%time predictions = herbarium_prototypes.predict(test_loader, proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\htngu\\Anaconda3\\envs\\kaggle_pandas\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5850699669753378"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(test_df['class_id'], predictions[0]\n",
    "               , average='macro'\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine-grained classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original competition, there are 140k test images. The test vectors then correspond to a 140k x 512 matrix, where 512 is the size of the embedding space. Classification of these images then consists of first taking pairwise distances between the 32k prototypes vectors and the 140k embedding vectors, giving a 140k x 32k matrix of distances. The classes of the images are argmax by row of the matrix of distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our GPU can't store all these distances at once, so instead we're computing prototypes on batches of classes at a time. For one batch of classes, we get a minimal distance and its corresponding class. By the end, we get say 50 candidates of (min distance, class) and just take the minimal of those. We won't be able to get top-5 accuracy this way but I struggled to find ways to lighten the memory load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first, we load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filepath</th>\n",
       "      <th>height</th>\n",
       "      <th>id</th>\n",
       "      <th>license</th>\n",
       "      <th>width</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60405</td>\n",
       "      <td>images/000/0.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131083</td>\n",
       "      <td>images/000/1.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>673</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122596</td>\n",
       "      <td>images/000/2.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82493</td>\n",
       "      <td>images/000/3.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92157</td>\n",
       "      <td>images/000/4.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index          filepath  height  id  license  width  class_id\n",
       "0   60405  images/000/0.jpg    1000   0        1    667         0\n",
       "1  131083  images/000/1.jpg    1000   1        1    673         0\n",
       "2  122596  images/000/2.jpg    1000   2        1    674         0\n",
       "3   82493  images/000/3.jpg    1000   3        1    667         0\n",
       "4   92157  images/000/4.jpg    1000   4        1    676         0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_dir + metadata_file, encoding = \"ISO-8859-1\") as json_file:\n",
    "    test_metadata = json.load(json_file)\n",
    "\n",
    "test_df = pd.DataFrame(test_metadata['images'])\n",
    "test_df['class_id'] = 0\n",
    "test_df.rename(columns={'file_name': 'filepath'}, inplace=True)\n",
    "test_df.sort_values('id', inplace=True)\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138292, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = myHerbariumDataset(test_df, train=False\n",
    "                                  , aug=False\n",
    "                                  , base_folder=test_dir\n",
    "                                  \n",
    "                                 )\n",
    "test_loader = DataLoader(test_dataset, batch_size=128\n",
    "                         , shuffle=False\n",
    "                         , num_workers=0\n",
    "                         , pin_memory=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we perform classifications in batches of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:15,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:17,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:33,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:13,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:13,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:13,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:13,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:13,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:15,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "Computing prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:14,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "n_class_batch = 32\n",
    "np_distance = np.zeros((test_df.shape[0], n_class_batch))\n",
    "np_class = np.zeros((test_df.shape[0], n_class_batch))\n",
    "batch_index = 0\n",
    "for class_batch in (np.array_split(np.arange(train_df.class_id.nunique()), n_class_batch)):\n",
    "    print(batch_index)\n",
    "    temp_support_df = train_df[train_df.class_id.isin(class_batch)].reset_index(drop=True)\n",
    "    support_dataset = myHerbariumDataset(temp_support_df\n",
    "                                         , train=False, aug=False\n",
    "                                         , base_folder=train_dir)\n",
    "    support_loader = DataLoader(support_dataset, batch_size=64\n",
    "                                , shuffle=False\n",
    "                                , num_workers=0\n",
    "                               )\n",
    "    herbarium_prototypes = Prototypes(model, temp_support_df, support_loader)\n",
    "    temp_class, temp_distance = herbarium_prototypes.predict(test_loader=test_loader, proba=False)\n",
    "    np_class[:, batch_index] = temp_class\n",
    "    np_distance[:, batch_index] = temp_distance\n",
    "    batch_index+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the final argmin and corresponding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_argmin = np.argmin(np_distance, axis=1)\n",
    "predicted_labels = [int(np_class[i, class_argmin[i]]) for i in range(np_class.shape[0] )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Predicted'] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>id</th>\n",
       "      <th>filepath</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156</td>\n",
       "      <td>256853</td>\n",
       "      <td>images/001/56/256853.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>675</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181</td>\n",
       "      <td>909534</td>\n",
       "      <td>images/001/81/909534.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>667</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>223562</td>\n",
       "      <td>images/002/00/223562.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>667</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151</td>\n",
       "      <td>239409</td>\n",
       "      <td>images/001/51/239409.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>682</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>1012718</td>\n",
       "      <td>images/000/71/1012718.jpg</td>\n",
       "      <td>1000</td>\n",
       "      <td>682</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_id       id                   filepath  height  width  Predicted\n",
       "0       156   256853   images/001/56/256853.jpg    1000    675        156\n",
       "1       181   909534   images/001/81/909534.jpg    1000    667        184\n",
       "2       200   223562   images/002/00/223562.jpg    1000    667        200\n",
       "3       151   239409   images/001/51/239409.jpg    1000    682        151\n",
       "4        71  1012718  images/000/71/1012718.jpg    1000    682         71"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the file if needed.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_df[['id','Predicted']]\n",
    " .rename(columns={'id':'Id'})\n",
    " .to_csv(base_dir + 'submission_seresnet_epoch50.csv', index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
